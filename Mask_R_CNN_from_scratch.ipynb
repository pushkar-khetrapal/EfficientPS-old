{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask R-CNN from scratch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushkar-khetrapal/EfficientPS/blob/master/Mask_R_CNN_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwJqn7i9U_ci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "9b0d4655-0fc0-4eeb-8a6a-9f5e8d4758ed"
      },
      "source": [
        "!pip install efficientnet_pytorch\n",
        "!pip install git+https://github.com/mapillary/inplace_abn\n",
        "!pip install torch-summary\n",
        "!git clone https://github.com/multimodallearning/pytorch-mask-rcnn.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/cb/0309a6e3d404862ae4bc017f89645cf150ac94c14c88ef81d215c8e52925/efficientnet_pytorch-0.6.3.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.5.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-cp36-none-any.whl size=12422 sha256=11f21052656b95e2a118ac7ee73026f9429f250ace5ccc0159213b6bdd7a4fa6\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/1e/a9/2a578ba9ad04e776e80bf0f70d8a7f4c29ec0718b92d8f6ccd\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.3\n",
            "Collecting git+https://github.com/mapillary/inplace_abn\n",
            "  Cloning https://github.com/mapillary/inplace_abn to /tmp/pip-req-build-z20rkvag\n",
            "  Running command git clone -q https://github.com/mapillary/inplace_abn /tmp/pip-req-build-z20rkvag\n",
            "Building wheels for collected packages: inplace-abn\n",
            "  Building wheel for inplace-abn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for inplace-abn: filename=inplace_abn-1.0.12-cp36-cp36m-linux_x86_64.whl size=3207545 sha256=1bcde87898afd51f55644ebefbdb2061eb29dc33ddfe0f8365f210cc52f16957\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-srnxo3a_/wheels/fe/0b/49/1303ca37166cc1be8784e2367a172133634dcd864a9df0ab56\n",
            "Successfully built inplace-abn\n",
            "Installing collected packages: inplace-abn\n",
            "Successfully installed inplace-abn-1.0.12\n",
            "Collecting torch-summary\n",
            "  Downloading https://files.pythonhosted.org/packages/34/c9/8b07f274404f08361710d971fde3164577c5f0ef984ecb1dd6e9e0541f99/torch_summary-1.3.3-py3-none-any.whl\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.3.3\n",
            "Cloning into 'pytorch-mask-rcnn'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mid9BTafn4ET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import maskRcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a0F2rBmpdBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIOZO5SVl3OA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "7defb9a2-9e7b-4732-ae22-eb62813e1fef"
      },
      "source": [
        "!pip uninstall scipy\n",
        "!pip install scipy==1.2.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling scipy-1.4.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/scipy-1.4.1.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/scipy/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled scipy-1.4.1\n",
            "Collecting scipy==1.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/69/20c8f3b7efe362093dff891239551ff90d4c463b5f52676e2694fea09442/scipy-1.2.2-cp36-cp36m-manylinux1_x86_64.whl (24.8MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8MB 129kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.2) (1.18.5)\n",
            "\u001b[31mERROR: umap-learn 0.4.4 has requirement scipy>=1.3.1, but you'll have scipy 1.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "Successfully installed scipy-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRBsWUqBcCci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from maskRcnn import maskutils\n",
        "from maskRcnn import visualize\n",
        "from maskRcnn.config import Config\n",
        "#from nms.nms_wrapper import nms\n",
        "#from roialign.roi_align.crop_and_resize import CropAndResizeFunction\n",
        "\n",
        "\n",
        "####################################################################################\n",
        "################# setting hyper-Parameters EfficientNet B5 #########################\n",
        "\n",
        "from efficientnet_pytorch import utils\n",
        "import collections\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "from torch import nn\n",
        "from inplace_abn.abn import InPlaceABN\n",
        "\n",
        "from efficientnet_pytorch.utils import (\n",
        "    round_filters,\n",
        "    round_repeats,\n",
        "    drop_connect,\n",
        "    get_same_padding_conv2d,\n",
        "    get_model_params,\n",
        "    efficientnet_params,\n",
        "    load_pretrained_weights,\n",
        "    Swish,\n",
        "    MemoryEfficientSwish,\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGaWu3Xdf5Vg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.ops import nms, roi_align"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApFYIJ5pOcIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlwFOVJKRg3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################\n",
        "#  Logging Utility Functions\n",
        "############################################################\n",
        "\n",
        "def log(text, array=None):\n",
        "    \"\"\"Prints a text message. And, optionally, if a Numpy array is provided it\n",
        "    prints it's shape, min, and max values.\n",
        "    \"\"\"\n",
        "    if array is not None:\n",
        "        text = text.ljust(25)\n",
        "        text += (\"shape: {:20}  min: {:10.5f}  max: {:10.5f}\".format(\n",
        "            str(array.shape),\n",
        "            array.min() if array.size else \"\",\n",
        "            array.max() if array.size else \"\"))\n",
        "    print(text)\n",
        "\n",
        "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n",
        "    \"\"\"\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\n')\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total:\n",
        "        print()\n",
        "\n",
        "\n",
        "############################################################\n",
        "#  Pytorch Utility Functions\n",
        "############################################################\n",
        "\n",
        "def unique1d(tensor):\n",
        "    if tensor.size()[0] == 0 or tensor.size()[0] == 1:\n",
        "        return tensor\n",
        "    tensor = tensor.sort()[0]\n",
        "    unique_bool = tensor[1:] != tensor [:-1]\n",
        "    first_element = Variable(torch.ByteTensor([True]), requires_grad=False)\n",
        "    if tensor.is_cuda:\n",
        "        first_element = first_element.cuda()\n",
        "    unique_bool = torch.cat((first_element, unique_bool),dim=0)\n",
        "    return tensor[unique_bool.data]\n",
        "\n",
        "def intersect1d(tensor1, tensor2):\n",
        "    aux = torch.cat((tensor1, tensor2),dim=0)\n",
        "    aux = aux.sort()[0]\n",
        "    return aux[:-1][(aux[1:] == aux[:-1]).data]\n",
        "\n",
        "def log2(x):\n",
        "    \"\"\"Implementatin of Log2. Pytorch doesn't have a native implemenation.\"\"\"\n",
        "    ln2 = Variable(torch.log(torch.FloatTensor([2.0])), requires_grad=False)\n",
        "    if x.is_cuda:\n",
        "        ln2 = ln2.cuda()\n",
        "    return torch.log(x) / ln2\n",
        "\n",
        "class SamePad2d(nn.Module):\n",
        "    \"\"\"Mimics tensorflow's 'SAME' padding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super(SamePad2d, self).__init__()\n",
        "        self.kernel_size = torch.nn.modules.utils._pair(kernel_size)\n",
        "        self.stride = torch.nn.modules.utils._pair(stride)\n",
        "\n",
        "    def forward(self, input):\n",
        "        in_width = input.size()[2]\n",
        "        in_height = input.size()[3]\n",
        "        out_width = math.ceil(float(in_width) / float(self.stride[0]))\n",
        "        out_height = math.ceil(float(in_height) / float(self.stride[1]))\n",
        "        pad_along_width = ((out_width - 1) * self.stride[0] +\n",
        "                           self.kernel_size[0] - in_width)\n",
        "        pad_along_height = ((out_height - 1) * self.stride[1] +\n",
        "                            self.kernel_size[1] - in_height)\n",
        "        pad_left = math.floor(pad_along_width / 2)\n",
        "        pad_top = math.floor(pad_along_height / 2)\n",
        "        pad_right = pad_along_width - pad_left\n",
        "        pad_bottom = pad_along_height - pad_top\n",
        "        return F.pad(input, (pad_left, pad_right, pad_top, pad_bottom), 'constant', 0)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__\n",
        "\n",
        "\n",
        "\n",
        "############################################################\n",
        "#  setting hyper-Parameters EfficientNet B5\n",
        "############################################################\n",
        "\n",
        "\n",
        "# Parameters for the entire model (stem, all blocks, and head)\n",
        "GlobalParams = collections.namedtuple('GlobalParams', [\n",
        "    'width_coefficient', 'depth_coefficient', 'image_size', 'dropout_rate',\n",
        "    'num_classes', 'batch_norm_momentum', 'batch_norm_epsilon',\n",
        "    'drop_connect_rate', 'depth_divisor', 'min_depth'])\n",
        "\n",
        "# Parameters for an individual model block\n",
        "BlockArgs = collections.namedtuple('BlockArgs', [\n",
        "    'num_repeat', 'kernel_size', 'stride', 'expand_ratio',\n",
        "    'input_filters', 'output_filters', 'se_ratio', 'id_skip'])\n",
        "\n",
        "\n",
        "def efficientnet_params(model_name):\n",
        "    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n",
        "    params_dict = {\n",
        "        # Coefficients:   width,depth,res,dropout\n",
        "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
        "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
        "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
        "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
        "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
        "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
        "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
        "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
        "        'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n",
        "        'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n",
        "    }\n",
        "    return params_dict[model_name]\n",
        "\n",
        "class BlockDecoder(object):\n",
        "    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _decode_block_string(block_string):\n",
        "        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n",
        "        assert isinstance(block_string, str)\n",
        "\n",
        "        ops = block_string.split('_')\n",
        "        options = {}\n",
        "        for op in ops:\n",
        "            splits = re.split(r'(\\d.*)', op)\n",
        "            if len(splits) >= 2:\n",
        "                key, value = splits[:2]\n",
        "                options[key] = value\n",
        "\n",
        "        # Check stride\n",
        "        assert (('s' in options and len(options['s']) == 1) or\n",
        "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
        "\n",
        "        return BlockArgs(\n",
        "            kernel_size=int(options['k']),\n",
        "            num_repeat=int(options['r']),\n",
        "            input_filters=int(options['i']),\n",
        "            output_filters=int(options['o']),\n",
        "            expand_ratio=int(options['e']),\n",
        "            id_skip=('noskip' not in block_string),\n",
        "            se_ratio=float(options['se']) if 'se' in options else None,\n",
        "            stride=[int(options['s'][0])])\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_block_string(block):\n",
        "        \"\"\"Encodes a block to a string.\"\"\"\n",
        "        args = [\n",
        "            'r%d' % block.num_repeat,\n",
        "            'k%d' % block.kernel_size,\n",
        "            's%d%d' % (block.strides[0], block.strides[1]),\n",
        "            'e%s' % block.expand_ratio,\n",
        "            'i%d' % block.input_filters,\n",
        "            'o%d' % block.output_filters\n",
        "        ]\n",
        "        if 0 < block.se_ratio <= 1:\n",
        "            args.append('se%s' % block.se_ratio)\n",
        "        if block.id_skip is False:\n",
        "            args.append('noskip')\n",
        "        return '_'.join(args)\n",
        "\n",
        "    @staticmethod\n",
        "    def decode(string_list):\n",
        "        \"\"\"\n",
        "        Decodes a list of string notations to specify blocks inside the network.\n",
        "\n",
        "        :param string_list: a list of strings, each string is a notation of block\n",
        "        :return: a list of BlockArgs namedtuples of block args\n",
        "        \"\"\"\n",
        "        assert isinstance(string_list, list)\n",
        "        blocks_args = []\n",
        "        for block_string in string_list:\n",
        "            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n",
        "        return blocks_args\n",
        "\n",
        "    @staticmethod\n",
        "    def encode(blocks_args):\n",
        "        \"\"\"\n",
        "        Encodes a list of BlockArgs to a list of strings.\n",
        "\n",
        "        :param blocks_args: a list of BlockArgs namedtuples of block args\n",
        "        :return: a list of strings, each string is a notation of block\n",
        "        \"\"\"\n",
        "        block_strings = []\n",
        "        for block in blocks_args:\n",
        "            block_strings.append(BlockDecoder._encode_block_string(block))\n",
        "        return block_strings\n",
        "\n",
        "\n",
        "def efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n",
        "                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n",
        "    \"\"\" Creates a efficientnet model. \"\"\"\n",
        "\n",
        "    blocks_args = [\n",
        "        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n",
        "        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n",
        "        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n",
        "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
        "    ]\n",
        "    blocks_args = BlockDecoder.decode(blocks_args)\n",
        "\n",
        "    global_params = GlobalParams(\n",
        "        batch_norm_momentum=0.99,\n",
        "        batch_norm_epsilon=1e-3,\n",
        "        dropout_rate=dropout_rate,\n",
        "        drop_connect_rate=drop_connect_rate,\n",
        "        # data_format='channels_last',  # removed, this is always true in PyTorch\n",
        "        num_classes=num_classes,\n",
        "        width_coefficient=width_coefficient,\n",
        "        depth_coefficient=depth_coefficient,\n",
        "        depth_divisor=8,\n",
        "        min_depth=None,\n",
        "        image_size=image_size,\n",
        "    )\n",
        "\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "def get_model_params(model_name, override_params):\n",
        "    \"\"\" Get the block args and global params for a given model \"\"\"\n",
        "    if model_name.startswith('efficientnet'):\n",
        "        w, d, s, p = efficientnet_params(model_name)\n",
        "        # note: all models have drop connect rate = 0.2\n",
        "        blocks_args, global_params = efficientnet(\n",
        "            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n",
        "    else:\n",
        "        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
        "    if override_params:\n",
        "        # ValueError will be raised here if override_params has fields not included in global_params.\n",
        "        global_params = global_params._replace(**override_params)\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "\n",
        "############################################################\n",
        "#  Efficient B-05 Graph\n",
        "############################################################\n",
        "\n",
        "########################\n",
        "###### MB-Blocks #######\n",
        "\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Mobile Inverted Residual Bottleneck Block\n",
        "\n",
        "    Args:\n",
        "        block_args (namedtuple): BlockArgs, see above\n",
        "        global_params (namedtuple): GlobalParam, see above\n",
        "\n",
        "    Attributes:\n",
        "        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block_args, global_params):\n",
        "        super().__init__()\n",
        "        self._block_args = block_args\n",
        "        self._bn_mom = 1 - global_params.batch_norm_momentum\n",
        "        self._bn_eps = global_params.batch_norm_epsilon\n",
        "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
        "        self.id_skip = block_args.id_skip  # skip connection and drop connect\n",
        "\n",
        "        # Get static or dynamic convolution depending on image size\n",
        "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
        "\n",
        "        # Expansion phase\n",
        "        inp = self._block_args.input_filters  # number of input channels\n",
        "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
        "            self._bn0 = InPlaceABN(oup)\n",
        "\n",
        "        # Depthwise convolution phase\n",
        "        k = self._block_args.kernel_size\n",
        "        s = self._block_args.stride\n",
        "        self._depthwise_conv = Conv2d(\n",
        "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
        "            kernel_size=k, stride=s, bias=False)\n",
        "        self._bn1 = InPlaceABN(oup)\n",
        "\n",
        "        # Squeeze and Excitation layer, if desired\n",
        "\n",
        "        ## Deleted it from here\n",
        "\n",
        "        # Output phase\n",
        "        final_oup = self._block_args.output_filters\n",
        "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
        "        self._bn2 = InPlaceABN(final_oup)\n",
        "\n",
        "    def forward(self, inputs, drop_connect_rate=None):\n",
        "        \"\"\"\n",
        "        :param inputs: input tensor\n",
        "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
        "        :return: output of block\n",
        "        \"\"\"\n",
        "\n",
        "        # Expansion and Depthwise Convolution\n",
        "        x = inputs\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            x = self._bn0(self._expand_conv(inputs))\n",
        "        x = self._bn1(self._depthwise_conv(x))\n",
        "\n",
        "        # Squeeze and Excitation\n",
        "        # Delete SE layer from here\n",
        "\n",
        "\n",
        "        x = self._bn2(self._project_conv(x))\n",
        "\n",
        "        # Skip connection and drop connect\n",
        "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
        "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
        "            if drop_connect_rate:\n",
        "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
        "            x = x + inputs  # skip connection\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    \"\"\"\n",
        "    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n",
        "\n",
        "    Args:\n",
        "        blocks_args (list): A list of BlockArgs to construct blocks\n",
        "        global_params (namedtuple): A set of GlobalParams shared between blocks\n",
        "\n",
        "    Example:\n",
        "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, blocks_args=None, global_params=None):\n",
        "        super().__init__()\n",
        "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
        "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
        "        self._global_params = global_params\n",
        "        self._blocks_args = blocks_args\n",
        "\n",
        "\n",
        "        # Build blocks\n",
        "        self._blocks = nn.ModuleList([])\n",
        "        self._array = []\n",
        "        for block_args in self._blocks_args:\n",
        "\n",
        "            self.temp = []\n",
        "            # Update block input and output filters based on depth multiplier.\n",
        "            block_args = block_args._replace(\n",
        "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
        "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
        "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
        "            )\n",
        "\n",
        "            # The first block needs to take care of stride and filter size increase.\n",
        "            self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
        "            self.temp.append(MBConvBlock(block_args, self._global_params))\n",
        "            if block_args.num_repeat > 1:\n",
        "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
        "            for _ in range(block_args.num_repeat - 1):\n",
        "                self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
        "                self.temp.append(MBConvBlock(block_args, self._global_params))\n",
        "            self._array.append( nn.Sequential(*self.temp))\n",
        "\n",
        "\n",
        "    def extract_features(self, inputs):\n",
        "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
        "            x = block(inputs, drop_connect_rate=drop_connect_rate)\n",
        "            \n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
        "        bs = inputs.size(0)\n",
        "        # Convolution layers\n",
        "        x = self.extract_features(inputs)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    ######## returning the middle blocks \n",
        "    def return_sub(self):\n",
        "      return self._array\n",
        "\n",
        "############################################################\n",
        "#  FPN Graph\n",
        "############################################################\n",
        "\n",
        "class SeparableConv2d(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size,stride=1,padding=1,dilation=1,bias=False):\n",
        "        super(SeparableConv2d,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FPN(nn.Module):\n",
        "     \n",
        "  def __init__(self, blocks,blocks_args=None, global_params=None):\n",
        "    super().__init__()\n",
        "\n",
        "    assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
        "    assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
        "    self._global_params = global_params\n",
        "    self._blocks_args = blocks_args\n",
        "\n",
        "    # Get static or dynamic convolution depending on image size\n",
        "    Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
        "\n",
        "    # Stem\n",
        "    self._conv_stem = Conv2d(3, 48, kernel_size=3, stride=2, bias=False)\n",
        "    self._bn0 = InPlaceABN(48)\n",
        "\n",
        "    #blocks\n",
        "    self.blocks0 = blocks[0]\n",
        "    self.blocks1 = blocks[1]\n",
        "    self.blocks2 = blocks[2]\n",
        "    self.blocks3 = blocks[3]\n",
        "    self.blocks4 = blocks[4]\n",
        "    self.blocks5 = blocks[5]\n",
        "    self.blocks6 = blocks[6]\n",
        "\n",
        "    # Head\n",
        "    self._conv_head = Conv2d(512, 2048, kernel_size=1, bias=False)\n",
        "    self._bn1 = InPlaceABN(2048)\n",
        "\n",
        "    same_Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
        "\n",
        "    # upper pyramid\n",
        "    self.conv_up1 = same_Conv2d(40, 256, kernel_size=1, stride=1, bias=False)\n",
        "    self.conv_up2 = same_Conv2d(64, 256, kernel_size=1, stride=1, bias=False)\n",
        "    self.conv_up3 = same_Conv2d(176, 256, kernel_size=1, stride=1, bias=False)\n",
        "    self.conv_up4 = same_Conv2d(2048, 256, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "    self.inABNone = InPlaceABN(256)\n",
        "    self.inABNtwo = InPlaceABN(256)\n",
        "    self.inABNthree = InPlaceABN(256)\n",
        "    self.inABNfour = InPlaceABN(256)\n",
        "\n",
        "    #separable\n",
        "\n",
        "    self.separable1 = SeparableConv2d(256, 256, 3)\n",
        "    self.separable2 = SeparableConv2d(256, 256, 3)\n",
        "    self.separable3 = SeparableConv2d(256, 256, 3)\n",
        "    self.separable4 = SeparableConv2d(256, 256, 3)\n",
        "\n",
        "    self.SepinABNone = InPlaceABN(256)\n",
        "    self.SepinABNtwo = InPlaceABN(256)\n",
        "    self.SepinABNthree = InPlaceABN(256)\n",
        "    self.SepinABNfour = InPlaceABN(256)\n",
        "\n",
        "    # upsample bilinear\n",
        "\n",
        "    self.up1 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "    self.up2 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "    self.up3 = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "\n",
        "\n",
        "    # downsample\n",
        "\n",
        "    self.down1 = nn.MaxPool2d(2, stride=2)\n",
        "    self.down2 = nn.MaxPool2d(2, stride=2)\n",
        "    self.down3 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "    \n",
        "  def forward(self, x):\n",
        "\n",
        "      # Stem\n",
        "      x = self._bn0(self._conv_stem(x))\n",
        "      # Blocks\n",
        "      x = self.blocks0(x)\n",
        "      x1 = self.blocks1(x)\n",
        "      x2 = self.blocks2(x1)\n",
        "      x = self.blocks3(x2)\n",
        "      x3 = self.blocks4(x)\n",
        "      x = self.blocks5(x3)\n",
        "      x = self.blocks6(x)\n",
        "\n",
        "      # Head\n",
        "      x4 = self._bn1(self._conv_head(x))\n",
        "      \n",
        "      #pyramids\n",
        "\n",
        "      u1 = self.inABNone(self.conv_up1(x1))\n",
        "      u2 = self.inABNtwo(self.conv_up2(x2))\n",
        "      u3 = self.inABNthree(self.conv_up3(x3))\n",
        "      u4 = self.inABNfour(self.conv_up4(x4))\n",
        "      \n",
        "      uu1 = self.down1(u1) \n",
        "      \n",
        "      uu2 = self.down2(uu1) + u3\n",
        "      uu3 = self.down3(uu2) + u4\n",
        "      \n",
        "      low1 = uu3 + u4\n",
        "      final1 = self.SepinABNone(self.separable1(low1))\n",
        "\n",
        "      low2 = u3 + self.up1(u4)\n",
        "      final2 = self.SepinABNtwo(self.separable2(low2 + uu2))\n",
        "\n",
        "      low3 = u2 + self.up2(low2)\n",
        "      final3 = self.SepinABNthree(self.separable3(low3 + uu1))\n",
        "\n",
        "      low4 = u1 + self.up3(low3)\n",
        "      final4 = self.SepinABNfour(self.separable4(low4 + u1))\n",
        "      \n",
        "      return [final4, final3, final2, final1]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp4J9GOXbywV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################\n",
        "#  Proposal Layer\n",
        "############################################################\n",
        "\n",
        "def apply_box_deltas(boxes, deltas):\n",
        "    \"\"\"Applies the given deltas to the given boxes.\n",
        "    boxes: [N, 4] where each row is y1, x1, y2, x2\n",
        "    deltas: [N, 4] where each row is [dy, dx, log(dh), log(dw)]\n",
        "    \"\"\"\n",
        "    # Convert to y, x, h, w\n",
        "    height = boxes[:, 2] - boxes[:, 0]\n",
        "    width = boxes[:, 3] - boxes[:, 1]\n",
        "    center_y = boxes[:, 0] + 0.5 * height\n",
        "    center_x = boxes[:, 1] + 0.5 * width\n",
        "    # Apply deltas\n",
        "    center_y += deltas[:, 0] * height\n",
        "    center_x += deltas[:, 1] * width\n",
        "    height *= torch.exp(deltas[:, 2])\n",
        "    width *= torch.exp(deltas[:, 3])\n",
        "    # Convert back to y1, x1, y2, x2\n",
        "    y1 = center_y - 0.5 * height\n",
        "    x1 = center_x - 0.5 * width\n",
        "    y2 = y1 + height\n",
        "    x2 = x1 + width\n",
        "    result = torch.stack([y1, x1, y2, x2], dim=1)\n",
        "    return result\n",
        "\n",
        "def clip_boxes(boxes, window):\n",
        "    \"\"\"\n",
        "    boxes: [N, 4] each col is y1, x1, y2, x2\n",
        "    window: [4] in the form y1, x1, y2, x2\n",
        "    \"\"\"\n",
        "    boxes = torch.stack( \\\n",
        "        [boxes[:, 0].clamp(float(window[0]), float(window[2])),\n",
        "         boxes[:, 1].clamp(float(window[1]), float(window[3])),\n",
        "         boxes[:, 2].clamp(float(window[0]), float(window[2])),\n",
        "         boxes[:, 3].clamp(float(window[1]), float(window[3]))], 1)\n",
        "    return boxes\n",
        "\n",
        "def proposal_layer(inputs, proposal_count, nms_threshold, anchors, config=None):\n",
        "    \"\"\"Receives anchor scores and selects a subset to pass as proposals\n",
        "    to the second stage. Filtering is done based on anchor scores and\n",
        "    non-max suppression to remove overlaps. It also applies bounding\n",
        "    box refinment detals to anchors.\n",
        "    Inputs:\n",
        "        rpn_probs: [batch, anchors, (bg prob, fg prob)]\n",
        "        rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\n",
        "    Returns:\n",
        "        Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)]\n",
        "    \"\"\"\n",
        "\n",
        "    # Currently only supports batchsize 1\n",
        "    inputs[0] = inputs[0].squeeze(0)\n",
        "    inputs[1] = inputs[1].squeeze(0)\n",
        "\n",
        "    # Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]\n",
        "    scores = inputs[0][:, 1]\n",
        "\n",
        "    # Box deltas [batch, num_rois, 4]\n",
        "    deltas = inputs[1]\n",
        "    std_dev = Variable(torch.from_numpy(np.reshape(config.RPN_BBOX_STD_DEV, [1, 4])).float(), requires_grad=False)\n",
        "    if config.GPU_COUNT:\n",
        "        std_dev = std_dev.cuda()\n",
        "    deltas = deltas * std_dev\n",
        "\n",
        "    # Improve performance by trimming to top anchors by score\n",
        "    # and doing the rest on the smaller subset.\n",
        "    pre_nms_limit = min(6000, anchors.size()[0])\n",
        "    scores, order = scores.sort(descending=True)\n",
        "    order = order[:pre_nms_limit]\n",
        "    scores = scores[:pre_nms_limit]\n",
        "    deltas = deltas[order.data, :] # TODO: Support batch size > 1 ff.\n",
        "    anchors = anchors[order.data, :]\n",
        "\n",
        "    # Apply deltas to anchors to get refined anchors.\n",
        "    # [batch, N, (y1, x1, y2, x2)]\n",
        "    boxes = apply_box_deltas(anchors, deltas)\n",
        "    \n",
        "    # Clip to image boundaries. [batch, N, (y1, x1, y2, x2)]\n",
        "    height, width = config.IMAGE_SHAPE[:2]\n",
        "    window = np.array([0, 0, height, width]).astype(np.float32)\n",
        "    boxes = clip_boxes(boxes, window)\n",
        "    print(boxes.shape)\n",
        "    print(scores.shape)\n",
        "    # Filter out small boxes\n",
        "    # According to Xinlei Chen's paper, this reduces detection accuracy\n",
        "    # for small objects, so we're skipping it.\n",
        "\n",
        "    # Non-max suppression\n",
        "    keep = nms(boxes, scores, nms_threshold)\n",
        "    keep = keep[:proposal_count]\n",
        "    boxes = boxes[keep, :]\n",
        "\n",
        "    # Normalize dimensions to range of 0 to 1.\n",
        "    norm = Variable(torch.from_numpy(np.array([height, width, height, width])).float(), requires_grad=False)\n",
        "    if config.GPU_COUNT:\n",
        "        norm = norm.cuda()\n",
        "    normalized_boxes = boxes / norm\n",
        "    print(boxes.shape)\n",
        "    \n",
        "    # Add back batch dimension\n",
        "    normalized_boxes = normalized_boxes.unsqueeze(0)\n",
        "\n",
        "    return normalized_boxes\n",
        "\n",
        "\n",
        "############################################################\n",
        "#  ROIAlign Layer\n",
        "############################################################\n",
        "\n",
        "def pyramid_roi_align(inputs, pool_size, image_shape):\n",
        "    \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid.\n",
        "    Params:\n",
        "    - pool_size: [height, width] of the output pooled regions. Usually [7, 7]\n",
        "    - image_shape: [height, width, channels]. Shape of input image in pixels\n",
        "    Inputs:\n",
        "    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized\n",
        "             coordinates.\n",
        "    - Feature maps: List of feature maps from different levels of the pyramid.\n",
        "                    Each is [batch, channels, height, width]\n",
        "    Output:\n",
        "    Pooled regions in the shape: [num_boxes, height, width, channels].\n",
        "    The width and height are those specific in the pool_shape in the layer\n",
        "    constructor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Currently only supports batchsize 1\n",
        "    for i in range(len(inputs)):\n",
        "        inputs[i] = inputs[i].squeeze(0)\n",
        "\n",
        "    # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n",
        "    boxes = inputs[0]\n",
        "\n",
        "    # Feature Maps. List of feature maps from different level of the\n",
        "    # feature pyramid. Each is [batch, height, width, channels]\n",
        "    feature_maps = inputs[1:]\n",
        "\n",
        "    # Assign each ROI to a level in the pyramid based on the ROI area.\n",
        "    y1, x1, y2, x2 = boxes.chunk(4, dim=1)\n",
        "    h = y2 - y1\n",
        "    w = x2 - x1\n",
        "\n",
        "    # Equation 1 in the Feature Pyramid Networks paper. Account for\n",
        "    # the fact that our coordinates are normalized here.\n",
        "    # e.g. a 224x224 ROI (in pixels) maps to P4\n",
        "    image_area = Variable(torch.FloatTensor([float(image_shape[0]*image_shape[1])]), requires_grad=False)\n",
        "    if boxes.is_cuda:\n",
        "        image_area = image_area.cuda()\n",
        "    roi_level = 4 + log2(torch.sqrt(h*w)/(224.0/torch.sqrt(image_area)))\n",
        "    roi_level = roi_level.round().int()\n",
        "    roi_level = roi_level.clamp(2,5)\n",
        "\n",
        "\n",
        "    # Loop through levels and apply ROI pooling to each. P2 to P5.\n",
        "    pooled = []\n",
        "    box_to_level = []\n",
        "    for i, level in enumerate(range(2, 6)):\n",
        "        ix  = roi_level==level\n",
        "        if not ix.any():\n",
        "            continue\n",
        "        ix = torch.nonzero(ix)[:,0]\n",
        "        level_boxes = boxes[ix.data, :]\n",
        "\n",
        "        # Keep track of which box is mapped to which level\n",
        "        box_to_level.append(ix.data)\n",
        "\n",
        "        # Stop gradient propogation to ROI proposals\n",
        "        level_boxes = level_boxes.detach()\n",
        "\n",
        "        # Crop and Resize\n",
        "        # From Mask R-CNN paper: \"We sample four regular locations, so\n",
        "        # that we can evaluate either max or average pooling. In fact,\n",
        "        # interpolating only a single value at each bin center (without\n",
        "        # pooling) is nearly as effective.\"\n",
        "        #\n",
        "        # Here we use the simplified approach of a single value per bin,\n",
        "        # which is how it's done in tf.crop_and_resize()\n",
        "        # Result: [batch * num_boxes, pool_height, pool_width, channels]\n",
        "        ind = Variable(torch.zeros(level_boxes.size()[0]),requires_grad=False).int()\n",
        "        if level_boxes.is_cuda:\n",
        "            ind = ind.cuda()\n",
        "        feature_maps[i] = feature_maps[i].unsqueeze(0)  #CropAndResizeFunction needs batch dimension\n",
        "        print(pool_size)\n",
        "        print(feature_maps[i].shape)\n",
        "        print(level_boxes.shape)\n",
        "        print(ind.shape)\n",
        "        pooled_features = roi_align(feature_maps[i], [level_boxes], (pool_size, pool_size))\n",
        "        pooled.append(pooled_features)\n",
        "\n",
        "    # Pack pooled features into one tensor\n",
        "    pooled = torch.cat(pooled, dim=0)\n",
        "\n",
        "    # Pack box_to_level mapping into one array and add another\n",
        "    # column representing the order of pooled boxes\n",
        "    box_to_level = torch.cat(box_to_level, dim=0)\n",
        "\n",
        "    # Rearrange pooled features to match the order of the original boxes\n",
        "    _, box_to_level = torch.sort(box_to_level)\n",
        "    pooled = pooled[box_to_level, :, :]\n",
        "    #print(pooled)\n",
        "    return pooled\n",
        "\n",
        "\n",
        "############################################################\n",
        "#  Detection Target Layer\n",
        "############################################################\n",
        "def bbox_overlaps(boxes1, boxes2):\n",
        "    \"\"\"Computes IoU overlaps between two sets of boxes.\n",
        "    boxes1, boxes2: [N, (y1, x1, y2, x2)].\n",
        "    \"\"\"\n",
        "    # 1. Tile boxes2 and repeate boxes1. This allows us to compare\n",
        "    # every boxes1 against every boxes2 without loops.\n",
        "    # TF doesn't have an equivalent to np.repeate() so simulate it\n",
        "    # using tf.tile() and tf.reshape.\n",
        "    boxes1_repeat = boxes2.size()[0]\n",
        "    boxes2_repeat = boxes1.size()[0]\n",
        "    boxes1 = boxes1.repeat(1,boxes1_repeat).view(-1,4)\n",
        "    boxes2 = boxes2.repeat(boxes2_repeat,1)\n",
        "\n",
        "    # 2. Compute intersections\n",
        "    b1_y1, b1_x1, b1_y2, b1_x2 = boxes1.chunk(4, dim=1)\n",
        "    b2_y1, b2_x1, b2_y2, b2_x2 = boxes2.chunk(4, dim=1)\n",
        "    y1 = torch.max(b1_y1, b2_y1)[:, 0]\n",
        "    x1 = torch.max(b1_x1, b2_x1)[:, 0]\n",
        "    y2 = torch.min(b1_y2, b2_y2)[:, 0]\n",
        "    x2 = torch.min(b1_x2, b2_x2)[:, 0]\n",
        "    zeros = Variable(torch.zeros(y1.size()[0]), requires_grad=False)\n",
        "    if y1.is_cuda:\n",
        "        zeros = zeros.cuda()\n",
        "    intersection = torch.max(x2 - x1, zeros) * torch.max(y2 - y1, zeros)\n",
        "\n",
        "    # 3. Compute unions\n",
        "    b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)\n",
        "    b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)\n",
        "    union = b1_area[:,0] + b2_area[:,0] - intersection\n",
        "\n",
        "    # 4. Compute IoU and reshape to [boxes1, boxes2]\n",
        "    iou = intersection / union\n",
        "    overlaps = iou.view(boxes2_repeat, boxes1_repeat)\n",
        "\n",
        "    return overlaps\n",
        "\n",
        "def detection_target_layer(proposals, gt_class_ids, gt_boxes, gt_masks, config):\n",
        "    \"\"\"Subsamples proposals and generates target box refinment, class_ids,\n",
        "    and masks for each.\n",
        "    Inputs:\n",
        "    proposals: [batch, N, (y1, x1, y2, x2)] in normalized coordinates. Might\n",
        "               be zero padded if there are not enough proposals.\n",
        "    gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs.\n",
        "    gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in normalized\n",
        "              coordinates.\n",
        "    gt_masks: [batch, height, width, MAX_GT_INSTANCES] of boolean type\n",
        "    Returns: Target ROIs and corresponding class IDs, bounding box shifts,\n",
        "    and masks.\n",
        "    rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized\n",
        "          coordinates\n",
        "    target_class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs.\n",
        "    target_deltas: [batch, TRAIN_ROIS_PER_IMAGE, NUM_CLASSES,\n",
        "                    (dy, dx, log(dh), log(dw), class_id)]\n",
        "                   Class-specific bbox refinments.\n",
        "    target_mask: [batch, TRAIN_ROIS_PER_IMAGE, height, width)\n",
        "                 Masks cropped to bbox boundaries and resized to neural\n",
        "                 network output size.\n",
        "    \"\"\"\n",
        "\n",
        "    # Currently only supports batchsize 1\n",
        "    proposals = proposals.squeeze(0)\n",
        "    gt_class_ids = gt_class_ids.squeeze(0)\n",
        "    gt_boxes = gt_boxes.squeeze(0)\n",
        "    gt_masks = gt_masks.squeeze(0)\n",
        "\n",
        "    # Handle COCO crowds\n",
        "    # A crowd box in COCO is a bounding box around several instances. Exclude\n",
        "    # them from training. A crowd box is given a negative class ID.\n",
        "    if torch.nonzero(gt_class_ids < 0).size():\n",
        "        crowd_ix = torch.nonzero(gt_class_ids < 0)[:, 0]\n",
        "        non_crowd_ix = torch.nonzero(gt_class_ids > 0)[:, 0]\n",
        "        crowd_boxes = gt_boxes[crowd_ix.data, :]\n",
        "        crowd_masks = gt_masks[crowd_ix.data, :, :]\n",
        "        gt_class_ids = gt_class_ids[non_crowd_ix.data]\n",
        "        gt_boxes = gt_boxes[non_crowd_ix.data, :]\n",
        "        gt_masks = gt_masks[non_crowd_ix.data, :]\n",
        "\n",
        "        # Compute overlaps with crowd boxes [anchors, crowds]\n",
        "        crowd_overlaps = bbox_overlaps(proposals, crowd_boxes)\n",
        "        crowd_iou_max = torch.max(crowd_overlaps, dim=1)[0]\n",
        "        no_crowd_bool = crowd_iou_max < 0.001\n",
        "    else:\n",
        "        no_crowd_bool =  Variable(torch.ByteTensor(proposals.size()[0]*[True]), requires_grad=False)\n",
        "        if config.GPU_COUNT:\n",
        "            no_crowd_bool = no_crowd_bool.cuda()\n",
        "\n",
        "    # Compute overlaps matrix [proposals, gt_boxes]\n",
        "    overlaps = bbox_overlaps(proposals, gt_boxes)\n",
        "\n",
        "    # Determine postive and negative ROIs\n",
        "    roi_iou_max = torch.max(overlaps, dim=1)[0]\n",
        "\n",
        "    # 1. Positive ROIs are those with >= 0.5 IoU with a GT box\n",
        "    positive_roi_bool = roi_iou_max >= 0.5\n",
        "\n",
        "    # Subsample ROIs. Aim for 33% positive\n",
        "    # Positive ROIs\n",
        "    if torch.nonzero(positive_roi_bool).size():\n",
        "        positive_indices = torch.nonzero(positive_roi_bool)[:, 0]\n",
        "\n",
        "        positive_count = int(config.TRAIN_ROIS_PER_IMAGE *\n",
        "                             config.ROI_POSITIVE_RATIO)\n",
        "        rand_idx = torch.randperm(positive_indices.size()[0])\n",
        "        rand_idx = rand_idx[:positive_count]\n",
        "        if config.GPU_COUNT:\n",
        "            rand_idx = rand_idx.cuda()\n",
        "        positive_indices = positive_indices[rand_idx]\n",
        "        positive_count = positive_indices.size()[0]\n",
        "        positive_rois = proposals[positive_indices.data,:]\n",
        "\n",
        "        # Assign positive ROIs to GT boxes.\n",
        "        positive_overlaps = overlaps[positive_indices.data,:]\n",
        "        roi_gt_box_assignment = torch.max(positive_overlaps, dim=1)[1]\n",
        "        roi_gt_boxes = gt_boxes[roi_gt_box_assignment.data,:]\n",
        "        roi_gt_class_ids = gt_class_ids[roi_gt_box_assignment.data]\n",
        "\n",
        "        # Compute bbox refinement for positive ROIs\n",
        "        deltas = Variable(maskutils.box_refinement(positive_rois.data, roi_gt_boxes.data), requires_grad=False)\n",
        "        std_dev = Variable(torch.from_numpy(config.BBOX_STD_DEV).float(), requires_grad=False)\n",
        "        if config.GPU_COUNT:\n",
        "            std_dev = std_dev.cuda()\n",
        "        deltas /= std_dev\n",
        "\n",
        "        # Assign positive ROIs to GT masks\n",
        "        roi_masks = gt_masks[roi_gt_box_assignment.data,:,:]\n",
        "\n",
        "        # Compute mask targets\n",
        "        boxes = positive_rois\n",
        "        if config.USE_MINI_MASK:\n",
        "            # Transform ROI corrdinates from normalized image space\n",
        "            # to normalized mini-mask space.\n",
        "            y1, x1, y2, x2 = positive_rois.chunk(4, dim=1)\n",
        "            gt_y1, gt_x1, gt_y2, gt_x2 = roi_gt_boxes.chunk(4, dim=1)\n",
        "            gt_h = gt_y2 - gt_y1\n",
        "            gt_w = gt_x2 - gt_x1\n",
        "            y1 = (y1 - gt_y1) / gt_h\n",
        "            x1 = (x1 - gt_x1) / gt_w\n",
        "            y2 = (y2 - gt_y1) / gt_h\n",
        "            x2 = (x2 - gt_x1) / gt_w\n",
        "            boxes = torch.cat([y1, x1, y2, x2], dim=1)\n",
        "        box_ids = Variable(torch.arange(roi_masks.size()[0]), requires_grad=False).int()\n",
        "        if config.GPU_COUNT:\n",
        "            box_ids = box_ids.cuda()\n",
        "        masks = Variable(CropAndResizeFunction(config.MASK_SHAPE[0], config.MASK_SHAPE[1], 0)(roi_masks.unsqueeze(1), boxes, box_ids).data, requires_grad=False)\n",
        "        masks = masks.squeeze(1)\n",
        "\n",
        "        # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with\n",
        "        # binary cross entropy loss.\n",
        "        masks = torch.round(masks)\n",
        "    else:\n",
        "        positive_count = 0\n",
        "\n",
        "    # 2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.\n",
        "    negative_roi_bool = roi_iou_max < 0.5\n",
        "    negative_roi_bool = negative_roi_bool & no_crowd_bool\n",
        "    # Negative ROIs. Add enough to maintain positive:negative ratio.\n",
        "    if torch.nonzero(negative_roi_bool).size() and positive_count>0:\n",
        "        negative_indices = torch.nonzero(negative_roi_bool)[:, 0]\n",
        "        r = 1.0 / config.ROI_POSITIVE_RATIO\n",
        "        negative_count = int(r * positive_count - positive_count)\n",
        "        rand_idx = torch.randperm(negative_indices.size()[0])\n",
        "        rand_idx = rand_idx[:negative_count]\n",
        "        if config.GPU_COUNT:\n",
        "            rand_idx = rand_idx.cuda()\n",
        "        negative_indices = negative_indices[rand_idx]\n",
        "        negative_count = negative_indices.size()[0]\n",
        "        negative_rois = proposals[negative_indices.data, :]\n",
        "    else:\n",
        "        negative_count = 0\n",
        "\n",
        "    # Append negative ROIs and pad bbox deltas and masks that\n",
        "    # are not used for negative ROIs with zeros.\n",
        "    if positive_count > 0 and negative_count > 0:\n",
        "        rois = torch.cat((positive_rois, negative_rois), dim=0)\n",
        "        zeros = Variable(torch.zeros(negative_count), requires_grad=False).int()\n",
        "        if config.GPU_COUNT:\n",
        "            zeros = zeros.cuda()\n",
        "        roi_gt_class_ids = torch.cat([roi_gt_class_ids, zeros], dim=0)\n",
        "        zeros = Variable(torch.zeros(negative_count,4), requires_grad=False)\n",
        "        if config.GPU_COUNT:\n",
        "            zeros = zeros.cuda()\n",
        "        deltas = torch.cat([deltas, zeros], dim=0)\n",
        "        zeros = Variable(torch.zeros(negative_count,config.MASK_SHAPE[0],config.MASK_SHAPE[1]), requires_grad=False)\n",
        "        if config.GPU_COUNT:\n",
        "            zeros = zeros.cuda()\n",
        "        masks = torch.cat([masks, zeros], dim=0)\n",
        "    elif positive_count > 0:\n",
        "        rois = positive_rois\n",
        "    elif negative_count > 0:\n",
        "        rois = negative_rois\n",
        "        zeros = Variable(torch.zeros(negative_count), requires_grad=False)\n",
        "        if config.GPU_COUNT:\n",
        "            zeros = zeros.cuda()\n",
        "        roi_gt_class_ids = zeros\n",
        "        zeros = Variable(torch.zeros(negative_count,4), requires_grad=False).int()\n",
        "        if config.GPU_COUNT:\n",
        "            zeros = zeros.cuda()\n",
        "        deltas = zeros\n",
        "        zeros = Variable(torch.zeros(negative_count,config.MASK_SHAPE[0],config.MASK_SHAPE[1]), requires_grad=False)\n",
        "        if config.GPU_COUNT:\n",
        "            zeros = zeros.cuda()\n",
        "        masks = zeros\n",
        "    else:\n",
        "        rois = Variable(torch.FloatTensor(), requires_grad=False)\n",
        "        roi_gt_class_ids = Variable(torch.IntTensor(), requires_grad=False)\n",
        "        deltas = Variable(torch.FloatTensor(), requires_grad=False)\n",
        "        masks = Variable(torch.FloatTensor(), requires_grad=False)\n",
        "        if config.GPU_COUNT:\n",
        "            rois = rois.cuda()\n",
        "            roi_gt_class_ids = roi_gt_class_ids.cuda()\n",
        "            deltas = deltas.cuda()\n",
        "            masks = masks.cuda()\n",
        "\n",
        "    return rois, roi_gt_class_ids, deltas, masks\n",
        "\n",
        "\n",
        "############################################################\n",
        "#  Detection Layer\n",
        "############################################################\n",
        "\n",
        "def clip_to_window(window, boxes):\n",
        "    \"\"\"\n",
        "        window: (y1, x1, y2, x2). The window in the image we want to clip to.\n",
        "        boxes: [N, (y1, x1, y2, x2)]\n",
        "    \"\"\"\n",
        "    boxes[:, 0] = boxes[:, 0].clamp(float(window[0]), float(window[2]))\n",
        "    boxes[:, 1] = boxes[:, 1].clamp(float(window[1]), float(window[3]))\n",
        "    boxes[:, 2] = boxes[:, 2].clamp(float(window[0]), float(window[2]))\n",
        "    boxes[:, 3] = boxes[:, 3].clamp(float(window[1]), float(window[3]))\n",
        "\n",
        "    return boxes\n",
        "\n",
        "def refine_detections(rois, probs, deltas, window, config):\n",
        "    \"\"\"Refine classified proposals and filter overlaps and return final\n",
        "    detections.\n",
        "    Inputs:\n",
        "        rois: [N, (y1, x1, y2, x2)] in normalized coordinates\n",
        "        probs: [N, num_classes]. Class probabilities.\n",
        "        deltas: [N, num_classes, (dy, dx, log(dh), log(dw))]. Class-specific\n",
        "                bounding box deltas.\n",
        "        window: (y1, x1, y2, x2) in image coordinates. The part of the image\n",
        "            that contains the image excluding the padding.\n",
        "    Returns detections shaped: [N, (y1, x1, y2, x2, class_id, score)]\n",
        "    \"\"\"\n",
        "\n",
        "    # Class IDs per ROI\n",
        "    _, class_ids = torch.max(probs, dim=1)\n",
        "\n",
        "    # Class probability of the top class of each ROI\n",
        "    # Class-specific bounding box deltas\n",
        "    idx = torch.arange(class_ids.size()[0]).long()\n",
        "    if config.GPU_COUNT:\n",
        "        idx = idx.cuda()\n",
        "    class_scores = probs[idx, class_ids.data]\n",
        "    deltas_specific = deltas[idx, class_ids.data]\n",
        "\n",
        "    # Apply bounding box deltas\n",
        "    # Shape: [boxes, (y1, x1, y2, x2)] in normalized coordinates\n",
        "    std_dev = Variable(torch.from_numpy(np.reshape(config.RPN_BBOX_STD_DEV, [1, 4])).float(), requires_grad=False)\n",
        "    if config.GPU_COUNT:\n",
        "        std_dev = std_dev.cuda()\n",
        "    refined_rois = apply_box_deltas(rois, deltas_specific * std_dev)\n",
        "\n",
        "    # Convert coordiates to image domain\n",
        "    height, width = config.IMAGE_SHAPE[:2]\n",
        "    scale = Variable(torch.from_numpy(np.array([height, width, height, width])).float(), requires_grad=False)\n",
        "    if config.GPU_COUNT:\n",
        "        scale = scale.cuda()\n",
        "    refined_rois *= scale\n",
        "\n",
        "    # Clip boxes to image window\n",
        "    refined_rois = clip_to_window(window, refined_rois)\n",
        "\n",
        "    # Round and cast to int since we're deadling with pixels now\n",
        "    refined_rois = torch.round(refined_rois)\n",
        "\n",
        "    # TODO: Filter out boxes with zero area\n",
        "\n",
        "    # Filter out background boxes\n",
        "    keep_bool = class_ids>0\n",
        "\n",
        "    # Filter out low confidence boxes\n",
        "    if config.DETECTION_MIN_CONFIDENCE:\n",
        "        keep_bool = keep_bool & (class_scores >= config.DETECTION_MIN_CONFIDENCE)\n",
        "    keep = torch.nonzero(keep_bool)[:,0]\n",
        "    print('aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa')\n",
        "    # Apply per-class NMS\n",
        "    pre_nms_class_ids = class_ids[keep.data]\n",
        "    pre_nms_scores = class_scores[keep.data]\n",
        "    pre_nms_rois = refined_rois[keep.data]\n",
        "\n",
        "    for i, class_id in enumerate(unique1d(pre_nms_class_ids)):\n",
        "        # Pick detections of this class\n",
        "        ixs = torch.nonzero(pre_nms_class_ids == class_id)[:,0]\n",
        "\n",
        "        # Sort\n",
        "        ix_rois = pre_nms_rois[ixs.data]\n",
        "        ix_scores = pre_nms_scores[ixs]\n",
        "        ix_scores, order = ix_scores.sort(descending=True)\n",
        "        ix_rois = ix_rois[order.data,:]\n",
        "        print('---------------------------------------')\n",
        "        print(ix_rois.shape)\n",
        "        print(ix_scores.shape)\n",
        "        class_keep = nms(ix_rois, ix_scores.unsqueeze(1), config.DETECTION_NMS_THRESHOLD)\n",
        "\n",
        "        # Map indicies\n",
        "        class_keep = keep[ixs[order[class_keep].data].data]\n",
        "  \n",
        "        if i==0:\n",
        "            nms_keep = class_keep\n",
        "        else:\n",
        "            nms_keep = unique1d(torch.cat((nms_keep, class_keep)))\n",
        "    keep = intersect1d(keep, nms_keep)\n",
        "\n",
        "    # Keep top detections\n",
        "    roi_count = config.DETECTION_MAX_INSTANCES\n",
        "    top_ids = class_scores[keep.data].sort(descending=True)[1][:roi_count]\n",
        "    keep = keep[top_ids.data]\n",
        "\n",
        "    # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\n",
        "    # Coordinates are in image domain.\n",
        "    result = torch.cat((refined_rois[keep.data],\n",
        "                        class_ids[keep.data].unsqueeze(1).float(),\n",
        "                        class_scores[keep.data].unsqueeze(1)), dim=1)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def detection_layer(config, rois, mrcnn_class, mrcnn_bbox, image_meta):\n",
        "    \"\"\"Takes classified proposal boxes and their bounding box deltas and\n",
        "    returns the final detection boxes.\n",
        "    Returns:\n",
        "    [batch, num_detections, (y1, x1, y2, x2, class_score)] in pixels\n",
        "    \"\"\"\n",
        "\n",
        "    # Currently only supports batchsize 1\n",
        "    rois = rois.squeeze(0)\n",
        "\n",
        "    _, _, window, _ = parse_image_meta(image_meta)\n",
        "    window = window[0]\n",
        "    detections = refine_detections(rois, mrcnn_class, mrcnn_bbox, window, config)\n",
        "\n",
        "    return detections\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcCsfVmlcFJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################\n",
        "#  Region Proposal Network\n",
        "############################################################\n",
        "\n",
        "class RPN(nn.Module):\n",
        "    \"\"\"Builds the model of Region Proposal Network.\n",
        "    anchors_per_location: number of anchors per pixel in the feature map\n",
        "    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n",
        "                   every pixel in the feature map), or 2 (every other pixel).\n",
        "    Returns:\n",
        "        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n",
        "        rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.\n",
        "        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n",
        "                  applied to anchors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, anchors_per_location, anchor_stride, depth):\n",
        "        super(RPN, self).__init__()\n",
        "        self.anchors_per_location = anchors_per_location\n",
        "        self.anchor_stride = anchor_stride\n",
        "        self.depth = depth\n",
        "\n",
        "        self.padding = SamePad2d(kernel_size=3, stride=self.anchor_stride)\n",
        "        self.conv_shared = nn.Conv2d(256, 256, kernel_size=3, stride=self.anchor_stride)\n",
        "        self.inABN = InPlaceABN(256)\n",
        "        self.conv_class = nn.Conv2d(256, 2 * anchors_per_location, kernel_size=1, stride=1)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "        self.conv_bbox = nn.Conv2d(256, 4 * anchors_per_location, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shared convolutional base of the RPN\n",
        "        x = self.inABN(self.conv_shared(self.padding(x)))\n",
        "\n",
        "        # Anchor Score. [batch, anchors per location * 2, height, width].\n",
        "        rpn_class_logits = self.conv_class(x)\n",
        "\n",
        "        # Reshape to [batch, 2, anchors]\n",
        "        rpn_class_logits = rpn_class_logits.permute(0,2,3,1)\n",
        "        rpn_class_logits = rpn_class_logits.contiguous()\n",
        "        rpn_class_logits = rpn_class_logits.view(x.size()[0], -1, 2)\n",
        "\n",
        "        # Softmax on last dimension of BG/FG.\n",
        "        rpn_probs = self.softmax(rpn_class_logits)\n",
        "\n",
        "        # Bounding box refinement. [batch, H, W, anchors per location, depth]\n",
        "        # where depth is [x, y, log(w), log(h)]\n",
        "        rpn_bbox = self.conv_bbox(x)\n",
        "\n",
        "        # Reshape to [batch, 4, anchors]\n",
        "        rpn_bbox = rpn_bbox.permute(0,2,3,1)\n",
        "        rpn_bbox = rpn_bbox.contiguous()\n",
        "        rpn_bbox = rpn_bbox.view(x.size()[0], -1, 4)\n",
        "\n",
        "        return [rpn_class_logits, rpn_probs, rpn_bbox]\n",
        "\n",
        "\n",
        "############################################################\n",
        "#  Feature Pyramid Network Heads\n",
        "############################################################\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, depth, pool_size, image_shape, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.pool_size = pool_size\n",
        "        self.image_shape = image_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # changed  \n",
        "        self.l1 = nn.Linear(14*14*256, 1024)\n",
        "        self.l2 = nn.Linear(1024, 1024)\n",
        "        self.inABN1 = InPlaceABN(1024)\n",
        "        self.inABN2 = InPlaceABN(1024)\n",
        "        #\n",
        "\n",
        "        self.linear_class = nn.Linear(1024, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.linear_bbox = nn.Linear(1024, num_classes * 4)\n",
        "\n",
        "    def forward(self, x, rois):\n",
        "        x = pyramid_roi_align([rois]+x, self.pool_size, self.image_shape)\n",
        "        \n",
        "        #changed\n",
        "        x = x.view(-1,14*14*256)\n",
        "        x = self.inABN1(self.l1(x))\n",
        "        x = self.inABN2(self.l2(x))\n",
        "        #\n",
        "\n",
        "        mrcnn_class_logits = self.linear_class(x)\n",
        "        mrcnn_probs = self.softmax(mrcnn_class_logits)\n",
        "\n",
        "        mrcnn_bbox = self.linear_bbox(x)\n",
        "        mrcnn_bbox = mrcnn_bbox.view(mrcnn_bbox.size()[0], -1, 4)\n",
        "\n",
        "        return [mrcnn_class_logits, mrcnn_probs, mrcnn_bbox]\n",
        "\n",
        "class Mask(nn.Module):\n",
        "    def __init__(self, depth, pool_size, image_shape, num_classes):\n",
        "        super(Mask, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.pool_size = pool_size\n",
        "        self.image_shape = image_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        ## changed here Conv2d -> separableConv2d and batchNorm -> inplaceABN\n",
        "        self.padding = SamePad2d(kernel_size=3, stride=1)\n",
        "        self.conv1 = SeparableConv2d(256, 256, 3)\n",
        "        self.bn1 = InPlaceABN(256)\n",
        "        self.conv2 = SeparableConv2d(256, 256, 3)\n",
        "        self.bn2 = InPlaceABN(256)\n",
        "        self.conv3 = SeparableConv2d(256, 256, 3)\n",
        "        self.bn3 = InPlaceABN(256)\n",
        "        self.conv4 = SeparableConv2d(256, 256, 3)\n",
        "        self.bn4 = InPlaceABN(256)\n",
        "        self.deconv = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n",
        "        self.bn5 = InPlaceABN(256)\n",
        "        self.conv5 = nn.Conv2d(256, num_classes, kernel_size=1, stride=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "       \n",
        "\n",
        "    def forward(self, x, rois):\n",
        "        x = pyramid_roi_align([rois] + x, self.pool_size, self.image_shape)\n",
        "        x = self.conv1(self.padding(x))\n",
        "        x = self.bn1(x)\n",
        "        x = self.conv2(self.padding(x))\n",
        "        x = self.bn2(x)\n",
        "        x = self.conv3(self.padding(x))\n",
        "        x = self.bn3(x)\n",
        "        x = self.conv4(self.padding(x))\n",
        "        x = self.bn4(x)\n",
        "        x = self.deconv(x)\n",
        "        x = self.bn5(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Gs13ZwccLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################\n",
        "#  Loss Functions\n",
        "############################################################\n",
        "\n",
        "def compute_rpn_class_loss(rpn_match, rpn_class_logits):\n",
        "    \"\"\"RPN anchor classifier loss.\n",
        "    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\n",
        "               -1=negative, 0=neutral anchor.\n",
        "    rpn_class_logits: [batch, anchors, 2]. RPN classifier logits for FG/BG.\n",
        "    \"\"\"\n",
        "\n",
        "    # Squeeze last dim to simplify\n",
        "    rpn_match = rpn_match.squeeze(2)\n",
        "\n",
        "    # Get anchor classes. Convert the -1/+1 match to 0/1 values.\n",
        "    anchor_class = (rpn_match == 1).long()\n",
        "\n",
        "    # Positive and Negative anchors contribute to the loss,\n",
        "    # but neutral anchors (match value = 0) don't.\n",
        "    indices = torch.nonzero(rpn_match != 0)\n",
        "\n",
        "    # Pick rows that contribute to the loss and filter out the rest.\n",
        "    rpn_class_logits = rpn_class_logits[indices.data[:,0],indices.data[:,1],:]\n",
        "    anchor_class = anchor_class[indices.data[:,0],indices.data[:,1]]\n",
        "\n",
        "    # Crossentropy loss\n",
        "    loss = F.cross_entropy(rpn_class_logits, anchor_class)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def compute_rpn_bbox_loss(target_bbox, rpn_match, rpn_bbox):\n",
        "    \"\"\"Return the RPN bounding box loss graph.\n",
        "    target_bbox: [batch, max positive anchors, (dy, dx, log(dh), log(dw))].\n",
        "        Uses 0 padding to fill in unsed bbox deltas.\n",
        "    rpn_match: [batch, anchors, 1]. Anchor match type. 1=positive,\n",
        "               -1=negative, 0=neutral anchor.\n",
        "    rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\n",
        "    \"\"\"\n",
        "\n",
        "    # Squeeze last dim to simplify\n",
        "    rpn_match = rpn_match.squeeze(2)\n",
        "\n",
        "    # Positive anchors contribute to the loss, but negative and\n",
        "    # neutral anchors (match value of 0 or -1) don't.\n",
        "    indices = torch.nonzero(rpn_match==1)\n",
        "\n",
        "    # Pick bbox deltas that contribute to the loss\n",
        "    rpn_bbox = rpn_bbox[indices.data[:,0],indices.data[:,1]]\n",
        "\n",
        "    # Trim target bounding box deltas to the same length as rpn_bbox.\n",
        "    target_bbox = target_bbox[0,:rpn_bbox.size()[0],:]\n",
        "\n",
        "    # Smooth L1 loss\n",
        "    loss = F.smooth_l1_loss(rpn_bbox, target_bbox)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_mrcnn_class_loss(target_class_ids, pred_class_logits):\n",
        "    \"\"\"Loss for the classifier head of Mask RCNN.\n",
        "    target_class_ids: [batch, num_rois]. Integer class IDs. Uses zero\n",
        "        padding to fill in the array.\n",
        "    pred_class_logits: [batch, num_rois, num_classes]\n",
        "    \"\"\"\n",
        "\n",
        "    # Loss\n",
        "    if target_class_ids.size():\n",
        "        loss = F.cross_entropy(pred_class_logits,target_class_ids.long())\n",
        "    else:\n",
        "        loss = Variable(torch.FloatTensor([0]), requires_grad=False)\n",
        "        if target_class_ids.is_cuda:\n",
        "            loss = loss.cuda()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_mrcnn_bbox_loss(target_bbox, target_class_ids, pred_bbox):\n",
        "    \"\"\"Loss for Mask R-CNN bounding box refinement.\n",
        "    target_bbox: [batch, num_rois, (dy, dx, log(dh), log(dw))]\n",
        "    target_class_ids: [batch, num_rois]. Integer class IDs.\n",
        "    pred_bbox: [batch, num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n",
        "    \"\"\"\n",
        "\n",
        "    if target_class_ids.size():\n",
        "        # Only positive ROIs contribute to the loss. And only\n",
        "        # the right class_id of each ROI. Get their indicies.\n",
        "        positive_roi_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
        "        positive_roi_class_ids = target_class_ids[positive_roi_ix.data].long()\n",
        "        indices = torch.stack((positive_roi_ix,positive_roi_class_ids), dim=1)\n",
        "\n",
        "        # Gather the deltas (predicted and true) that contribute to loss\n",
        "        target_bbox = target_bbox[indices[:,0].data,:]\n",
        "        pred_bbox = pred_bbox[indices[:,0].data,indices[:,1].data,:]\n",
        "\n",
        "        # Smooth L1 loss\n",
        "        loss = F.smooth_l1_loss(pred_bbox, target_bbox)\n",
        "    else:\n",
        "        loss = Variable(torch.FloatTensor([0]), requires_grad=False)\n",
        "        if target_class_ids.is_cuda:\n",
        "            loss = loss.cuda()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def compute_mrcnn_mask_loss(target_masks, target_class_ids, pred_masks):\n",
        "    \"\"\"Mask binary cross-entropy loss for the masks head.\n",
        "    target_masks: [batch, num_rois, height, width].\n",
        "        A float32 tensor of values 0 or 1. Uses zero padding to fill array.\n",
        "    target_class_ids: [batch, num_rois]. Integer class IDs. Zero padded.\n",
        "    pred_masks: [batch, proposals, height, width, num_classes] float32 tensor\n",
        "                with values from 0 to 1.\n",
        "    \"\"\"\n",
        "    if target_class_ids.size():\n",
        "        # Only positive ROIs contribute to the loss. And only\n",
        "        # the class specific mask of each ROI.\n",
        "        positive_ix = torch.nonzero(target_class_ids > 0)[:, 0]\n",
        "        positive_class_ids = target_class_ids[positive_ix.data].long()\n",
        "        indices = torch.stack((positive_ix, positive_class_ids), dim=1)\n",
        "\n",
        "        # Gather the masks (predicted and true) that contribute to loss\n",
        "        y_true = target_masks[indices[:,0].data,:,:]\n",
        "        y_pred = pred_masks[indices[:,0].data,indices[:,1].data,:,:]\n",
        "\n",
        "        # Binary cross entropy\n",
        "        loss = F.binary_cross_entropy(y_pred, y_true)\n",
        "    else:\n",
        "        loss = Variable(torch.FloatTensor([0]), requires_grad=False)\n",
        "        if target_class_ids.is_cuda:\n",
        "            loss = loss.cuda()\n",
        "\n",
        "    return loss\n",
        "\n",
        "def compute_losses(rpn_match, rpn_bbox, rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask):\n",
        "\n",
        "    rpn_class_loss = compute_rpn_class_loss(rpn_match, rpn_class_logits)\n",
        "    rpn_bbox_loss = compute_rpn_bbox_loss(rpn_bbox, rpn_match, rpn_pred_bbox)\n",
        "    mrcnn_class_loss = compute_mrcnn_class_loss(target_class_ids, mrcnn_class_logits)\n",
        "    mrcnn_bbox_loss = compute_mrcnn_bbox_loss(target_deltas, target_class_ids, mrcnn_bbox)\n",
        "    mrcnn_mask_loss = compute_mrcnn_mask_loss(target_mask, target_class_ids, mrcnn_mask)\n",
        "\n",
        "    return [rpn_class_loss, rpn_bbox_loss, mrcnn_class_loss, mrcnn_bbox_loss, mrcnn_mask_loss]\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYFiEosmcqf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################\n",
        "#  Data Generator\n",
        "############################################################\n",
        "\n",
        "def load_image_gt(dataset, config, image_id, augment=False,\n",
        "                  use_mini_mask=False):\n",
        "    \"\"\"Load and return ground truth data for an image (image, mask, bounding boxes).\n",
        "    augment: If true, apply random image augmentation. Currently, only\n",
        "        horizontal flipping is offered.\n",
        "    use_mini_mask: If False, returns full-size masks that are the same height\n",
        "        and width as the original image. These can be big, for example\n",
        "        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,\n",
        "        224x224 and are generated by extracting the bounding box of the\n",
        "        object and resizing it to MINI_MASK_SHAPE.\n",
        "    Returns:\n",
        "    image: [height, width, 3]\n",
        "    shape: the original shape of the image before resizing and cropping.\n",
        "    class_ids: [instance_count] Integer class IDs\n",
        "    bbox: [instance_count, (y1, x1, y2, x2)]\n",
        "    mask: [height, width, instance_count]. The height and width are those\n",
        "        of the image unless use_mini_mask is True, in which case they are\n",
        "        defined in MINI_MASK_SHAPE.\n",
        "    \"\"\"\n",
        "    # Load image and mask\n",
        "    image = dataset.load_image(image_id)\n",
        "    mask, class_ids = dataset.load_mask(image_id)\n",
        "    shape = image.shape\n",
        "    image, window, scale, padding = maskutils.resize_image(\n",
        "        image,\n",
        "        min_dim=config.IMAGE_MIN_DIM,\n",
        "        max_dim=config.IMAGE_MAX_DIM,\n",
        "        padding=config.IMAGE_PADDING)\n",
        "    mask = maskutils.resize_mask(mask, scale, padding)\n",
        "\n",
        "    # Random horizontal flips.\n",
        "    if augment:\n",
        "        if random.randint(0, 1):\n",
        "            image = np.fliplr(image)\n",
        "            mask = np.fliplr(mask)\n",
        "\n",
        "    # Bounding boxes. Note that some boxes might be all zeros\n",
        "    # if the corresponding mask got cropped out.\n",
        "    # bbox: [num_instances, (y1, x1, y2, x2)]\n",
        "    bbox = maskutils.extract_bboxes(mask)\n",
        "\n",
        "    # Active classes\n",
        "    # Different datasets have different classes, so track the\n",
        "    # classes supported in the dataset of this image.\n",
        "    active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)\n",
        "    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][\"source\"]]\n",
        "    active_class_ids[source_class_ids] = 1\n",
        "\n",
        "    # Resize masks to smaller size to reduce memory usage\n",
        "    if use_mini_mask:\n",
        "        mask = maskutils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
        "\n",
        "    # Image meta data\n",
        "    image_meta = compose_image_meta(image_id, shape, window, active_class_ids)\n",
        "\n",
        "    return image, image_meta, class_ids, bbox, mask\n",
        "\n",
        "def build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config):\n",
        "    \"\"\"Given the anchors and GT boxes, compute overlaps and identify positive\n",
        "    anchors and deltas to refine them to match their corresponding GT boxes.\n",
        "    anchors: [num_anchors, (y1, x1, y2, x2)]\n",
        "    gt_class_ids: [num_gt_boxes] Integer class IDs.\n",
        "    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]\n",
        "    Returns:\n",
        "    rpn_match: [N] (int32) matches between anchors and GT boxes.\n",
        "               1 = positive anchor, -1 = negative anchor, 0 = neutral\n",
        "    rpn_bbox: [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\n",
        "    \"\"\"\n",
        "    # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral\n",
        "    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)\n",
        "    # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]\n",
        "    rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))\n",
        "\n",
        "    # Handle COCO crowds\n",
        "    # A crowd box in COCO is a bounding box around several instances. Exclude\n",
        "    # them from training. A crowd box is given a negative class ID.\n",
        "    crowd_ix = np.where(gt_class_ids < 0)[0]\n",
        "    if crowd_ix.shape[0] > 0:\n",
        "        # Filter out crowds from ground truth class IDs and boxes\n",
        "        non_crowd_ix = np.where(gt_class_ids > 0)[0]\n",
        "        crowd_boxes = gt_boxes[crowd_ix]\n",
        "        gt_class_ids = gt_class_ids[non_crowd_ix]\n",
        "        gt_boxes = gt_boxes[non_crowd_ix]\n",
        "        # Compute overlaps with crowd boxes [anchors, crowds]\n",
        "        crowd_overlaps = maskutils.compute_overlaps(anchors, crowd_boxes)\n",
        "        crowd_iou_max = np.amax(crowd_overlaps, axis=1)\n",
        "        no_crowd_bool = (crowd_iou_max < 0.001)\n",
        "    else:\n",
        "        # All anchors don't intersect a crowd\n",
        "        no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)\n",
        "\n",
        "    # Compute overlaps [num_anchors, num_gt_boxes]\n",
        "    overlaps = maskutils.compute_overlaps(anchors, gt_boxes)\n",
        "\n",
        "    # Match anchors to GT Boxes\n",
        "    # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.\n",
        "    # If an anchor overlaps a GT box with IoU < 0.3 then it's negative.\n",
        "    # Neutral anchors are those that don't match the conditions above,\n",
        "    # and they don't influence the loss function.\n",
        "    # However, don't keep any GT box unmatched (rare, but happens). Instead,\n",
        "    # match it to the closest anchor (even if its max IoU is < 0.3).\n",
        "    #\n",
        "    # 1. Set negative anchors first. They get overwritten below if a GT box is\n",
        "    # matched to them. Skip boxes in crowd areas.\n",
        "    anchor_iou_argmax = np.argmax(overlaps, axis=1)\n",
        "    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]\n",
        "    rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1\n",
        "    # 2. Set an anchor for each GT box (regardless of IoU value).\n",
        "    # TODO: If multiple anchors have the same IoU match all of them\n",
        "    gt_iou_argmax = np.argmax(overlaps, axis=0)\n",
        "    rpn_match[gt_iou_argmax] = 1\n",
        "    # 3. Set anchors with high overlap as positive.\n",
        "    rpn_match[anchor_iou_max >= 0.7] = 1\n",
        "\n",
        "    # Subsample to balance positive and negative anchors\n",
        "    # Don't let positives be more than half the anchors\n",
        "    ids = np.where(rpn_match == 1)[0]\n",
        "    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)\n",
        "    if extra > 0:\n",
        "        # Reset the extra ones to neutral\n",
        "        ids = np.random.choice(ids, extra, replace=False)\n",
        "        rpn_match[ids] = 0\n",
        "    # Same for negative proposals\n",
        "    ids = np.where(rpn_match == -1)[0]\n",
        "    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE -\n",
        "                        np.sum(rpn_match == 1))\n",
        "    if extra > 0:\n",
        "        # Rest the extra ones to neutral\n",
        "        ids = np.random.choice(ids, extra, replace=False)\n",
        "        rpn_match[ids] = 0\n",
        "\n",
        "    # For positive anchors, compute shift and scale needed to transform them\n",
        "    # to match the corresponding GT boxes.\n",
        "    ids = np.where(rpn_match == 1)[0]\n",
        "    ix = 0  # index into rpn_bbox\n",
        "    # TODO: use box_refinment() rather than duplicating the code here\n",
        "    for i, a in zip(ids, anchors[ids]):\n",
        "        # Closest gt box (it might have IoU < 0.7)\n",
        "        gt = gt_boxes[anchor_iou_argmax[i]]\n",
        "\n",
        "        # Convert coordinates to center plus width/height.\n",
        "        # GT Box\n",
        "        gt_h = gt[2] - gt[0]\n",
        "        gt_w = gt[3] - gt[1]\n",
        "        gt_center_y = gt[0] + 0.5 * gt_h\n",
        "        gt_center_x = gt[1] + 0.5 * gt_w\n",
        "        # Anchor\n",
        "        a_h = a[2] - a[0]\n",
        "        a_w = a[3] - a[1]\n",
        "        a_center_y = a[0] + 0.5 * a_h\n",
        "        a_center_x = a[1] + 0.5 * a_w\n",
        "\n",
        "        # Compute the bbox refinement that the RPN should predict.\n",
        "        rpn_bbox[ix] = [\n",
        "            (gt_center_y - a_center_y) / a_h,\n",
        "            (gt_center_x - a_center_x) / a_w,\n",
        "            np.log(gt_h / a_h),\n",
        "            np.log(gt_w / a_w),\n",
        "        ]\n",
        "        # Normalize\n",
        "        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV\n",
        "        ix += 1\n",
        "\n",
        "    return rpn_match, rpn_bbox\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, config, augment=True):\n",
        "        \"\"\"A generator that returns images and corresponding target class ids,\n",
        "            bounding box deltas, and masks.\n",
        "            dataset: The Dataset object to pick data from\n",
        "            config: The model config object\n",
        "            shuffle: If True, shuffles the samples before every epoch\n",
        "            augment: If True, applies image augmentation to images (currently only\n",
        "                     horizontal flips are supported)\n",
        "            Returns a Python generator. Upon calling next() on it, the\n",
        "            generator returns two lists, inputs and outputs. The containtes\n",
        "            of the lists differs depending on the received arguments:\n",
        "            inputs list:\n",
        "            - images: [batch, H, W, C]\n",
        "            - image_metas: [batch, size of image meta]\n",
        "            - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)\n",
        "            - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\n",
        "            - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs\n",
        "            - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]\n",
        "            - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width\n",
        "                        are those of the image unless use_mini_mask is True, in which\n",
        "                        case they are defined in MINI_MASK_SHAPE.\n",
        "            outputs list: Usually empty in regular training. But if detection_targets\n",
        "                is True then the outputs list contains target class_ids, bbox deltas,\n",
        "                and masks.\n",
        "            \"\"\"\n",
        "        self.b = 0  # batch item index\n",
        "        self.image_index = -1\n",
        "        self.image_ids = np.copy(dataset.image_ids)\n",
        "        self.error_count = 0\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.config = config\n",
        "        self.augment = augment\n",
        "\n",
        "        # Anchors\n",
        "        # [anchor_count, (y1, x1, y2, x2)]\n",
        "        self.anchors = maskutils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
        "                                                 config.RPN_ANCHOR_RATIOS,\n",
        "                                                 config.BACKBONE_SHAPES,\n",
        "                                                 config.BACKBONE_STRIDES,\n",
        "                                                 config.RPN_ANCHOR_STRIDE)\n",
        "\n",
        "    def __getitem__(self, image_index):\n",
        "        # Get GT bounding boxes and masks for image.\n",
        "        image_id = self.image_ids[image_index]\n",
        "        image, image_metas, gt_class_ids, gt_boxes, gt_masks = \\\n",
        "            load_image_gt(self.dataset, self.config, image_id, augment=self.augment,\n",
        "                          use_mini_mask=self.config.USE_MINI_MASK)\n",
        "\n",
        "        # Skip images that have no instances. This can happen in cases\n",
        "        # where we train on a subset of classes and the image doesn't\n",
        "        # have any of the classes we care about.\n",
        "        if not np.any(gt_class_ids > 0):\n",
        "            return None\n",
        "\n",
        "        # RPN Targets\n",
        "        rpn_match, rpn_bbox = build_rpn_targets(image.shape, self.anchors,\n",
        "                                                gt_class_ids, gt_boxes, self.config)\n",
        "\n",
        "        # If more instances than fits in the array, sub-sample from them.\n",
        "        if gt_boxes.shape[0] > self.config.MAX_GT_INSTANCES:\n",
        "            ids = np.random.choice(\n",
        "                np.arange(gt_boxes.shape[0]), self.config.MAX_GT_INSTANCES, replace=False)\n",
        "            gt_class_ids = gt_class_ids[ids]\n",
        "            gt_boxes = gt_boxes[ids]\n",
        "            gt_masks = gt_masks[:, :, ids]\n",
        "\n",
        "        # Add to batch\n",
        "        rpn_match = rpn_match[:, np.newaxis]\n",
        "        images = mold_image(image.astype(np.float32), self.config)\n",
        "\n",
        "        # Convert\n",
        "        images = torch.from_numpy(images.transpose(2, 0, 1)).float()\n",
        "        image_metas = torch.from_numpy(image_metas)\n",
        "        rpn_match = torch.from_numpy(rpn_match)\n",
        "        rpn_bbox = torch.from_numpy(rpn_bbox).float()\n",
        "        gt_class_ids = torch.from_numpy(gt_class_ids)\n",
        "        gt_boxes = torch.from_numpy(gt_boxes).float()\n",
        "        gt_masks = torch.from_numpy(gt_masks.astype(int).transpose(2, 0, 1)).float()\n",
        "\n",
        "        return images, image_metas, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.image_ids.shape[0]\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cvrSL7_c06w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################\n",
        "#  MaskRCNN Class\n",
        "############################################################\n",
        "\n",
        "class MaskRCNN(nn.Module):\n",
        "    \"\"\"Encapsulates the Mask RCNN model functionality.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, model_dir = None):\n",
        "        \"\"\"\n",
        "        config: A Sub-class of the Config class\n",
        "        model_dir: Directory to save training logs and trained weights\n",
        "        \"\"\"\n",
        "        super(MaskRCNN, self).__init__()\n",
        "        self.config = config\n",
        "        self.model_dir = model_dir\n",
        "        self.set_log_dir()\n",
        "        self.build(config=config)\n",
        "        self.initialize_weights()\n",
        "        self.loss_history = []\n",
        "        self.val_loss_history = []\n",
        "        \n",
        "\n",
        "\n",
        "    def build(self, config):\n",
        "        \"\"\"Build Mask R-CNN architecture.\n",
        "        \"\"\"\n",
        "\n",
        "        # Image size must be dividable by 2 multiple times\n",
        "        h, w = config.IMAGE_SHAPE[:2]\n",
        "        if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n",
        "            raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
        "                            \"to avoid fractions when downscaling and upscaling.\"\n",
        "                            \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
        "\n",
        "        # Build the shared convolutional layers.\n",
        "        # Bottom-up Layers\n",
        "        # Returns a list of the last layers of each stage, 5 in total.\n",
        "        # Don't create the thead (stage 5), so we pick the 4th item in the list.\n",
        "\n",
        "        override_params={'num_classes': 1}\n",
        "        paras = get_model_params( 'efficientnet-b5', override_params )\n",
        "\n",
        "        efficientNetModel = EfficientNet(paras[0],paras[1])\n",
        "        arr = efficientNetModel.return_sub()\n",
        "        \n",
        "        # Top-down Layers\n",
        "        # TODO: add assert to varify feature map sizes match what's in config\n",
        "        \n",
        "        self.fpn = FPN( arr, paras[0], paras[1] )\n",
        "        self.fpn.to('cuda')\n",
        "\n",
        "\n",
        "\n",
        "        # Generate Anchors\n",
        "        self.anchors = Variable(torch.from_numpy(maskutils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
        "                                                                                config.RPN_ANCHOR_RATIOS,\n",
        "                                                                                config.BACKBONE_SHAPES,\n",
        "                                                                                config.BACKBONE_STRIDES,\n",
        "                                                                                config.RPN_ANCHOR_STRIDE)).float(), requires_grad=False)\n",
        "        if self.config.GPU_COUNT:\n",
        "            self.anchors = self.anchors.cuda()\n",
        "        \n",
        "        print(self.anchors)\n",
        "\n",
        "        # RPN\n",
        "        self.rpn = RPN(len(config.RPN_ANCHOR_RATIOS), config.RPN_ANCHOR_STRIDE, 256)\n",
        "\n",
        "        # FPN Classifier\n",
        "        self.classifier = Classifier(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
        "\n",
        "        # FPN Mask\n",
        "        self.mask = Mask(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
        "\n",
        "    def get_anchors(self):\n",
        "      return self.anchors\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        \"\"\"Initialize model weights.\n",
        "        \"\"\"\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def set_trainable(self, layer_regex, model=None, indent=0, verbose=1):\n",
        "        \"\"\"Sets model layers as trainable if their names match\n",
        "        the given regular expression.\n",
        "        \"\"\"\n",
        "\n",
        "        for param in self.named_parameters():\n",
        "            layer_name = param[0]\n",
        "            trainable = bool(re.fullmatch(layer_regex, layer_name))\n",
        "            if not trainable:\n",
        "                param[1].requires_grad = False\n",
        "\n",
        "    def set_log_dir(self, model_path=None):\n",
        "        \"\"\"Sets the model log directory and epoch counter.\n",
        "        model_path: If None, or a format different from what this code uses\n",
        "            then set a new log directory and start epochs from 0. Otherwise,\n",
        "            extract the log directory and the epoch counter from the file\n",
        "            name.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set date and epoch counter as if starting a new model\n",
        "        self.epoch = 0\n",
        "        now = datetime.datetime.now()\n",
        "\n",
        "        # If we have a model path with date and epochs use them\n",
        "        if model_path:\n",
        "            # Continue from we left of. Get epoch and date from the file name\n",
        "            # A sample model path might look like:\n",
        "            # /path/to/logs/coco20171029T2315/mask_rcnn_coco_0001.h5\n",
        "            regex = r\".*/\\w+(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})/mask\\_rcnn\\_\\w+(\\d{4})\\.pth\"\n",
        "            m = re.match(regex, model_path)\n",
        "            if m:\n",
        "                now = datetime.datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)),\n",
        "                                        int(m.group(4)), int(m.group(5)))\n",
        "                self.epoch = int(m.group(6))\n",
        "\n",
        "        # Directory for training logs\n",
        "        self.log_dir = os.path.join(self.model_dir, \"{}{:%Y%m%dT%H%M}\".format(\n",
        "            self.config.NAME.lower(), now))\n",
        "\n",
        "        # Path to save after each epoch. Include placeholders that get filled by Keras.\n",
        "        self.checkpoint_path = os.path.join(self.log_dir, \"mask_rcnn_{}_*epoch*.pth\".format(\n",
        "            self.config.NAME.lower()))\n",
        "        self.checkpoint_path = self.checkpoint_path.replace(\n",
        "            \"*epoch*\", \"{:04d}\")\n",
        "\n",
        "    def find_last(self):\n",
        "        \"\"\"Finds the last checkpoint file of the last trained model in the\n",
        "        model directory.\n",
        "        Returns:\n",
        "            log_dir: The directory where events and weights are saved\n",
        "            checkpoint_path: the path to the last checkpoint file\n",
        "        \"\"\"\n",
        "        # Get directory names. Each directory corresponds to a model\n",
        "        dir_names = next(os.walk(self.model_dir))[1]\n",
        "        key = self.config.NAME.lower()\n",
        "        dir_names = filter(lambda f: f.startswith(key), dir_names)\n",
        "        dir_names = sorted(dir_names)\n",
        "        if not dir_names:\n",
        "            return None, None\n",
        "        # Pick last directory\n",
        "        dir_name = os.path.join(self.model_dir, dir_names[-1])\n",
        "        # Find the last checkpoint\n",
        "        checkpoints = next(os.walk(dir_name))[2]\n",
        "        checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n",
        "        checkpoints = sorted(checkpoints)\n",
        "        if not checkpoints:\n",
        "            return dir_name, None\n",
        "        checkpoint = os.path.join(dir_name, checkpoints[-1])\n",
        "        return dir_name, checkpoint\n",
        "\n",
        "    def load_weights(self, filepath):\n",
        "        \"\"\"Modified version of the correspoding Keras function with\n",
        "        the addition of multi-GPU support and the ability to exclude\n",
        "        some layers from loading.\n",
        "        exlude: list of layer names to excluce\n",
        "        \"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            state_dict = torch.load(filepath)\n",
        "            self.load_state_dict(state_dict, strict=False)\n",
        "        else:\n",
        "            print(\"Weight file not found ...\")\n",
        "\n",
        "        # Update the log directory\n",
        "        self.set_log_dir(filepath)\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.makedirs(self.log_dir)\n",
        "\n",
        "    def detect(self, images):\n",
        "        \"\"\"Runs the detection pipeline.\n",
        "        images: List of images, potentially of different sizes.\n",
        "        Returns a list of dicts, one dict per image. The dict contains:\n",
        "        rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n",
        "        class_ids: [N] int class IDs\n",
        "        scores: [N] float probability scores for the class IDs\n",
        "        masks: [H, W, N] instance binary masks\n",
        "        \"\"\"\n",
        "\n",
        "        # Mold inputs to format expected by the neural network\n",
        "        molded_images, image_metas, windows = self.mold_inputs(images)\n",
        "\n",
        "        # Convert images to torch tensor\n",
        "        molded_images = torch.from_numpy(molded_images.transpose(0, 3, 1, 2)).float()\n",
        "\n",
        "        # To GPU\n",
        "        if self.config.GPU_COUNT:\n",
        "            molded_images = molded_images.cuda()\n",
        "\n",
        "        # Wrap in variable\n",
        "        molded_images = Variable(molded_images, volatile=True)\n",
        "\n",
        "        # Run object detection\n",
        "        detections, mrcnn_mask = self.predict([molded_images, image_metas], mode='inference')\n",
        "            # Convert to numpy\n",
        "        detections = detections.data.cpu().numpy()\n",
        "        mrcnn_mask = mrcnn_mask.permute(0, 1, 3, 4, 2).data.cpu().numpy()\n",
        "\n",
        "        # Process detections\n",
        "        results = []\n",
        "        for i, image in enumerate(images):\n",
        "            final_rois, final_class_ids, final_scores, final_masks =\\\n",
        "                self.unmold_detections(detections[i], mrcnn_mask[i],\n",
        "                                       image.shape, windows[i])\n",
        "            results.append({\n",
        "                \"rois\": final_rois,\n",
        "                \"class_ids\": final_class_ids,\n",
        "                \"scores\": final_scores,\n",
        "                \"masks\": final_masks,\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    def predict(self, input, mode):\n",
        "        molded_images = input[0]\n",
        "        image_metas = input[1]\n",
        "\n",
        "        if mode == 'inference':\n",
        "            self.eval()\n",
        "        elif mode == 'training':\n",
        "            self.train()\n",
        "\n",
        "\n",
        "        # Feature extraction\n",
        "        [p2_out, p3_out, p4_out, p5_out] = self.fpn(molded_images)\n",
        "\n",
        "        # Note that P6 is used in RPN, but not in the classifier heads.\n",
        "        rpn_feature_maps = [p2_out, p3_out, p4_out, p5_out]\n",
        "        mrcnn_feature_maps = [p2_out, p3_out, p4_out, p5_out]\n",
        "\n",
        "\n",
        "        # Loop through pyramid layers\n",
        "        layer_outputs = []  # list of lists\n",
        "        for p in rpn_feature_maps:\n",
        "            layer_outputs.append(self.rpn(p))\n",
        "\n",
        "        # Concatenate layer outputs\n",
        "        # Convert from list of lists of level outputs to list of lists\n",
        "        # of outputs across levels.\n",
        "        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
        "        outputs = list(zip(*layer_outputs))\n",
        "        outputs = [torch.cat(list(o), dim=1) for o in outputs]\n",
        "        rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
        "\n",
        "        # Generate proposals\n",
        "        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
        "        # and zero padded.\n",
        "        proposal_count = self.config.POST_NMS_ROIS_TRAINING if mode == \"training\" \\\n",
        "            else self.config.POST_NMS_ROIS_INFERENCE\n",
        "        rpn_rois = proposal_layer([rpn_class, rpn_bbox],\n",
        "                                 proposal_count=proposal_count,\n",
        "                                 nms_threshold=self.config.RPN_NMS_THRESHOLD,\n",
        "                                 anchors=self.anchors,\n",
        "                                 config=self.config)\n",
        "        if mode == 'inference':\n",
        "            # Network Heads\n",
        "            # Proposal classifier and BBox regressor heads\n",
        "            \n",
        "            mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rpn_rois)\n",
        "\n",
        "            # Detections\n",
        "            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates\n",
        "            detections = detection_layer(self.config, rpn_rois, mrcnn_class, mrcnn_bbox, image_metas)\n",
        "\n",
        "            # Convert boxes to normalized coordinates\n",
        "            # TODO: let DetectionLayer return normalized coordinates to avoid\n",
        "            #       unnecessary conversions\n",
        "            h, w = self.config.IMAGE_SHAPE[:2]\n",
        "            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
        "            if self.config.GPU_COUNT:\n",
        "                scale = scale.cuda()\n",
        "            detection_boxes = detections[:, :4] / scale\n",
        "\n",
        "            # Add back batch dimension\n",
        "            detection_boxes = detection_boxes.unsqueeze(0)\n",
        "\n",
        "            # Create masks for detections\n",
        "            mrcnn_mask = self.mask(mrcnn_feature_maps, detection_boxes)\n",
        "\n",
        "            # Add back batch dimension\n",
        "            detections = detections.unsqueeze(0)\n",
        "            mrcnn_mask = mrcnn_mask.unsqueeze(0)\n",
        "\n",
        "            return [detections, mrcnn_mask]\n",
        "\n",
        "        elif mode == 'training':\n",
        "\n",
        "            gt_class_ids = input[2]\n",
        "            gt_boxes = input[3]\n",
        "            gt_masks = input[4]\n",
        "\n",
        "            # Normalize coordinates\n",
        "            h, w = self.config.IMAGE_SHAPE[:2]\n",
        "            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
        "            if self.config.GPU_COUNT:\n",
        "                scale = scale.cuda()\n",
        "            gt_boxes = gt_boxes / scale\n",
        "\n",
        "            # Generate detection targets\n",
        "            # Subsamples proposals and generates target outputs for training\n",
        "            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n",
        "            # padded. Equally, returned rois and targets are zero padded.\n",
        "            rois, target_class_ids, target_deltas, target_mask = \\\n",
        "                detection_target_layer(rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config)\n",
        "\n",
        "            if not rois.size():\n",
        "                mrcnn_class_logits = Variable(torch.FloatTensor())\n",
        "                mrcnn_class = Variable(torch.IntTensor())\n",
        "                mrcnn_bbox = Variable(torch.FloatTensor())\n",
        "                mrcnn_mask = Variable(torch.FloatTensor())\n",
        "                if self.config.GPU_COUNT:\n",
        "                    mrcnn_class_logits = mrcnn_class_logits.cuda()\n",
        "                    mrcnn_class = mrcnn_class.cuda()\n",
        "                    mrcnn_bbox = mrcnn_bbox.cuda()\n",
        "                    mrcnn_mask = mrcnn_mask.cuda()\n",
        "            else:\n",
        "                # Network Heads\n",
        "                # Proposal classifier and BBox regressor heads\n",
        "                mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rois)\n",
        "\n",
        "                # Create masks for detections\n",
        "                mrcnn_mask = self.mask(mrcnn_feature_maps, rois)\n",
        "\n",
        "            return [rpn_class_logits, rpn_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask]\n",
        "      \n",
        "\n",
        "\n",
        "    def train_model(self, train_dataset, val_dataset, learning_rate, epochs, layers):\n",
        "        \"\"\"Train the model.\n",
        "        train_dataset, val_dataset: Training and validation Dataset objects.\n",
        "        learning_rate: The learning rate to train with\n",
        "        epochs: Number of training epochs. Note that previous training epochs\n",
        "                are considered to be done alreay, so this actually determines\n",
        "                the epochs to train in total rather than in this particaular\n",
        "                call.\n",
        "        layers: Allows selecting wich layers to train. It can be:\n",
        "            - A regular expression to match layer names to train\n",
        "            - One of these predefined values:\n",
        "              heaads: The RPN, classifier and mask heads of the network\n",
        "              all: All the layers\n",
        "              3+: Train Resnet stage 3 and up\n",
        "              4+: Train Resnet stage 4 and up\n",
        "              5+: Train Resnet stage 5 and up\n",
        "        \"\"\"\n",
        "\n",
        "        # Pre-defined layer regular expressions\n",
        "        layer_regex = {\n",
        "            # all layers but the backbone\n",
        "            \"heads\": r\"(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)\",\n",
        "            # From a specific Resnet stage and up\n",
        "            \"3+\": r\"(fpn.C3.*)|(fpn.C4.*)|(fpn.C5.*)|(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)\",\n",
        "            \"4+\": r\"(fpn.C4.*)|(fpn.C5.*)|(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)\",\n",
        "            \"5+\": r\"(fpn.C5.*)|(fpn.P5\\_.*)|(fpn.P4\\_.*)|(fpn.P3\\_.*)|(fpn.P2\\_.*)|(rpn.*)|(classifier.*)|(mask.*)\",\n",
        "            # All layers\n",
        "            \"all\": \".*\",\n",
        "        }\n",
        "        if layers in layer_regex.keys():\n",
        "            layers = layer_regex[layers]\n",
        "\n",
        "        # Data generators\n",
        "        train_set = Dataset(train_dataset, self.config, augment=True)\n",
        "        train_generator = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, num_workers=4)\n",
        "        val_set = Dataset(val_dataset, self.config, augment=True)\n",
        "        val_generator = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "        # Train\n",
        "        log(\"\\nStarting at epoch {}. LR={}\\n\".format(self.epoch+1, learning_rate))\n",
        "        log(\"Checkpoint Path: {}\".format(self.checkpoint_path))\n",
        "        self.set_trainable(layers)\n",
        "\n",
        "        # Optimizer object\n",
        "        # Add L2 Regularization\n",
        "        # Skip gamma and beta weights of batch normalization layers.\n",
        "        trainables_wo_bn = [param for name, param in self.named_parameters() if param.requires_grad and not 'bn' in name]\n",
        "        trainables_only_bn = [param for name, param in self.named_parameters() if param.requires_grad and 'bn' in name]\n",
        "        optimizer = optim.SGD([\n",
        "            {'params': trainables_wo_bn, 'weight_decay': self.config.WEIGHT_DECAY},\n",
        "            {'params': trainables_only_bn}\n",
        "        ], lr=learning_rate, momentum=self.config.LEARNING_MOMENTUM)\n",
        "\n",
        "        for epoch in range(self.epoch+1, epochs+1):\n",
        "            log(\"Epoch {}/{}.\".format(epoch,epochs))\n",
        "\n",
        "            # Training\n",
        "            loss, loss_rpn_class, loss_rpn_bbox, loss_mrcnn_class, loss_mrcnn_bbox, loss_mrcnn_mask = self.train_epoch(train_generator, optimizer, self.config.STEPS_PER_EPOCH)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_loss_rpn_class, val_loss_rpn_bbox, val_loss_mrcnn_class, val_loss_mrcnn_bbox, val_loss_mrcnn_mask = self.valid_epoch(val_generator, self.config.VALIDATION_STEPS)\n",
        "\n",
        "            # Statistics\n",
        "            self.loss_history.append([loss, loss_rpn_class, loss_rpn_bbox, loss_mrcnn_class, loss_mrcnn_bbox, loss_mrcnn_mask])\n",
        "            self.val_loss_history.append([val_loss, val_loss_rpn_class, val_loss_rpn_bbox, val_loss_mrcnn_class, val_loss_mrcnn_bbox, val_loss_mrcnn_mask])\n",
        "            visualize.plot_loss(self.loss_history, self.val_loss_history, save=True, log_dir=self.log_dir)\n",
        "\n",
        "            # Save model\n",
        "            torch.save(self.state_dict(), self.checkpoint_path.format(epoch))\n",
        "\n",
        "        self.epoch = epochs\n",
        "\n",
        "\n",
        "\n",
        "    def train_epoch(self, datagenerator, optimizer, steps):\n",
        "        batch_count = 0\n",
        "        loss_sum = 0\n",
        "        loss_rpn_class_sum = 0\n",
        "        loss_rpn_bbox_sum = 0\n",
        "        loss_mrcnn_class_sum = 0\n",
        "        loss_mrcnn_bbox_sum = 0\n",
        "        loss_mrcnn_mask_sum = 0\n",
        "        step = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for inputs in datagenerator:\n",
        "            batch_count += 1\n",
        "\n",
        "            images = inputs[0]\n",
        "            image_metas = inputs[1]\n",
        "            rpn_match = inputs[2]\n",
        "            rpn_bbox = inputs[3]\n",
        "            gt_class_ids = inputs[4]\n",
        "            gt_boxes = inputs[5]\n",
        "            gt_masks = inputs[6]\n",
        "\n",
        "            # image_metas as numpy array\n",
        "            image_metas = image_metas.numpy()\n",
        "\n",
        "            # Wrap in variables\n",
        "            images = Variable(images)\n",
        "            rpn_match = Variable(rpn_match)\n",
        "            rpn_bbox = Variable(rpn_bbox)\n",
        "            gt_class_ids = Variable(gt_class_ids)\n",
        "            gt_boxes = Variable(gt_boxes)\n",
        "            gt_masks = Variable(gt_masks)\n",
        "\n",
        "            # To GPU\n",
        "            if self.config.GPU_COUNT:\n",
        "                images = images.cuda()\n",
        "                rpn_match = rpn_match.cuda()\n",
        "                rpn_bbox = rpn_bbox.cuda()\n",
        "                gt_class_ids = gt_class_ids.cuda()\n",
        "                gt_boxes = gt_boxes.cuda()\n",
        "                gt_masks = gt_masks.cuda()\n",
        "\n",
        "            # Run object detection\n",
        "            rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask = \\\n",
        "                self.predict([images, image_metas, gt_class_ids, gt_boxes, gt_masks], mode='training')\n",
        "\n",
        "            # Compute losses\n",
        "            rpn_class_loss, rpn_bbox_loss, mrcnn_class_loss, mrcnn_bbox_loss, mrcnn_mask_loss = compute_losses(rpn_match, rpn_bbox, rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask)\n",
        "            loss = rpn_class_loss + rpn_bbox_loss + mrcnn_class_loss + mrcnn_bbox_loss + mrcnn_mask_loss\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm(self.parameters(), 5.0)\n",
        "            if (batch_count % self.config.BATCH_SIZE) == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                batch_count = 0\n",
        "\n",
        "            # Progress\n",
        "            printProgressBar(step + 1, steps, prefix=\"\\t{}/{}\".format(step + 1, steps),\n",
        "                             suffix=\"Complete - loss: {:.5f} - rpn_class_loss: {:.5f} - rpn_bbox_loss: {:.5f} - mrcnn_class_loss: {:.5f} - mrcnn_bbox_loss: {:.5f} - mrcnn_mask_loss: {:.5f}\".format(\n",
        "                                 loss.data.cpu()[0], rpn_class_loss.data.cpu()[0], rpn_bbox_loss.data.cpu()[0],\n",
        "                                 mrcnn_class_loss.data.cpu()[0], mrcnn_bbox_loss.data.cpu()[0],\n",
        "                                 mrcnn_mask_loss.data.cpu()[0]), length=10)\n",
        "\n",
        "            # Statistics\n",
        "            loss_sum += loss.data.cpu()[0]/steps\n",
        "            loss_rpn_class_sum += rpn_class_loss.data.cpu()[0]/steps\n",
        "            loss_rpn_bbox_sum += rpn_bbox_loss.data.cpu()[0]/steps\n",
        "            loss_mrcnn_class_sum += mrcnn_class_loss.data.cpu()[0]/steps\n",
        "            loss_mrcnn_bbox_sum += mrcnn_bbox_loss.data.cpu()[0]/steps\n",
        "            loss_mrcnn_mask_sum += mrcnn_mask_loss.data.cpu()[0]/steps\n",
        "\n",
        "            # Break after 'steps' steps\n",
        "            if step==steps-1:\n",
        "                break\n",
        "            step += 1\n",
        "\n",
        "        return loss_sum, loss_rpn_class_sum, loss_rpn_bbox_sum, loss_mrcnn_class_sum, loss_mrcnn_bbox_sum, loss_mrcnn_mask_sum\n",
        "\n",
        "    def valid_epoch(self, datagenerator, steps):\n",
        "\n",
        "        step = 0\n",
        "        loss_sum = 0\n",
        "        loss_rpn_class_sum = 0\n",
        "        loss_rpn_bbox_sum = 0\n",
        "        loss_mrcnn_class_sum = 0\n",
        "        loss_mrcnn_bbox_sum = 0\n",
        "        loss_mrcnn_mask_sum = 0\n",
        "\n",
        "        for inputs in datagenerator:\n",
        "            images = inputs[0]\n",
        "            image_metas = inputs[1]\n",
        "            rpn_match = inputs[2]\n",
        "            rpn_bbox = inputs[3]\n",
        "            gt_class_ids = inputs[4]\n",
        "            gt_boxes = inputs[5]\n",
        "            gt_masks = inputs[6]\n",
        "\n",
        "            # image_metas as numpy array\n",
        "            image_metas = image_metas.numpy()\n",
        "\n",
        "            # Wrap in variables\n",
        "            images = Variable(images, volatile=True)\n",
        "            rpn_match = Variable(rpn_match, volatile=True)\n",
        "            rpn_bbox = Variable(rpn_bbox, volatile=True)\n",
        "            gt_class_ids = Variable(gt_class_ids, volatile=True)\n",
        "            gt_boxes = Variable(gt_boxes, volatile=True)\n",
        "            gt_masks = Variable(gt_masks, volatile=True)\n",
        "\n",
        "            # To GPU\n",
        "            if self.config.GPU_COUNT:\n",
        "                images = images.cuda()\n",
        "                rpn_match = rpn_match.cuda()\n",
        "                rpn_bbox = rpn_bbox.cuda()\n",
        "                gt_class_ids = gt_class_ids.cuda()\n",
        "                gt_boxes = gt_boxes.cuda()\n",
        "                gt_masks = gt_masks.cuda()\n",
        "\n",
        "            # Run object detection\n",
        "            rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask = \\\n",
        "                self.predict([images, image_metas, gt_class_ids, gt_boxes, gt_masks], mode='training')\n",
        "\n",
        "            if not target_class_ids.size():\n",
        "                continue\n",
        "\n",
        "            # Compute losses\n",
        "            rpn_class_loss, rpn_bbox_loss, mrcnn_class_loss, mrcnn_bbox_loss, mrcnn_mask_loss = compute_losses(rpn_match, rpn_bbox, rpn_class_logits, rpn_pred_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask)\n",
        "            loss = rpn_class_loss + rpn_bbox_loss + mrcnn_class_loss + mrcnn_bbox_loss + mrcnn_mask_loss\n",
        "\n",
        "            # Progress\n",
        "            printProgressBar(step + 1, steps, prefix=\"\\t{}/{}\".format(step + 1, steps),\n",
        "                             suffix=\"Complete - loss: {:.5f} - rpn_class_loss: {:.5f} - rpn_bbox_loss: {:.5f} - mrcnn_class_loss: {:.5f} - mrcnn_bbox_loss: {:.5f} - mrcnn_mask_loss: {:.5f}\".format(\n",
        "                                 loss.data.cpu()[0], rpn_class_loss.data.cpu()[0], rpn_bbox_loss.data.cpu()[0],\n",
        "                                 mrcnn_class_loss.data.cpu()[0], mrcnn_bbox_loss.data.cpu()[0],\n",
        "                                 mrcnn_mask_loss.data.cpu()[0]), length=10)\n",
        "\n",
        "            # Statistics\n",
        "            loss_sum += loss.data.cpu()[0]/steps\n",
        "            loss_rpn_class_sum += rpn_class_loss.data.cpu()[0]/steps\n",
        "            loss_rpn_bbox_sum += rpn_bbox_loss.data.cpu()[0]/steps\n",
        "            loss_mrcnn_class_sum += mrcnn_class_loss.data.cpu()[0]/steps\n",
        "            loss_mrcnn_bbox_sum += mrcnn_bbox_loss.data.cpu()[0]/steps\n",
        "            loss_mrcnn_mask_sum += mrcnn_mask_loss.data.cpu()[0]/steps\n",
        "\n",
        "            # Break after 'steps' steps\n",
        "            if step==steps-1:\n",
        "                break\n",
        "            step += 1\n",
        "\n",
        "        return loss_sum, loss_rpn_class_sum, loss_rpn_bbox_sum, loss_mrcnn_class_sum, loss_mrcnn_bbox_sum, loss_mrcnn_mask_sum\n",
        "\n",
        "\n",
        "\n",
        "    def mold_inputs(self, images):\n",
        "        \"\"\"Takes a list of images and modifies them to the format expected\n",
        "        as an input to the neural network.\n",
        "        images: List of image matricies [height,width,depth]. Images can have\n",
        "            different sizes.\n",
        "        Returns 3 Numpy matricies:\n",
        "        molded_images: [N, h, w, 3]. Images resized and normalized.\n",
        "        image_metas: [N, length of meta data]. Details about each image.\n",
        "        windows: [N, (y1, x1, y2, x2)]. The portion of the image that has the\n",
        "            original image (padding excluded).\n",
        "        \"\"\"\n",
        "        molded_images = []\n",
        "        image_metas = []\n",
        "        windows = []\n",
        "        for image in images:\n",
        "            # Resize image to fit the model expected size\n",
        "            # TODO: move resizing to mold_image()\n",
        "            molded_image, window, scale, padding = maskutils.resize_image(\n",
        "                image,\n",
        "                min_dim=self.config.IMAGE_MIN_DIM,\n",
        "                max_dim=self.config.IMAGE_MAX_DIM,\n",
        "                padding=self.config.IMAGE_PADDING)\n",
        "            molded_image = mold_image(molded_image, self.config)\n",
        "            # Build image_meta\n",
        "            image_meta = compose_image_meta(\n",
        "                0, image.shape, window,\n",
        "                np.zeros([self.config.NUM_CLASSES], dtype=np.int32))\n",
        "            # Append\n",
        "            molded_images.append(molded_image)\n",
        "            windows.append(window)\n",
        "            image_metas.append(image_meta)\n",
        "        # Pack into arrays\n",
        "        molded_images = np.stack(molded_images)\n",
        "        image_metas = np.stack(image_metas)\n",
        "        windows = np.stack(windows)\n",
        "        return molded_images, image_metas, windows\n",
        "\n",
        "    def unmold_detections(self, detections, mrcnn_mask, image_shape, window):\n",
        "        \"\"\"Reformats the detections of one image from the format of the neural\n",
        "        network output to a format suitable for use in the rest of the\n",
        "        application.\n",
        "        detections: [N, (y1, x1, y2, x2, class_id, score)]\n",
        "        mrcnn_mask: [N, height, width, num_classes]\n",
        "        image_shape: [height, width, depth] Original size of the image before resizing\n",
        "        window: [y1, x1, y2, x2] Box in the image where the real image is\n",
        "                excluding the padding.\n",
        "        Returns:\n",
        "        boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels\n",
        "        class_ids: [N] Integer class IDs for each bounding box\n",
        "        scores: [N] Float probability scores of the class_id\n",
        "        masks: [height, width, num_instances] Instance masks\n",
        "        \"\"\"\n",
        "        # How many detections do we have?\n",
        "        # Detections array is padded with zeros. Find the first class_id == 0.\n",
        "        zero_ix = np.where(detections[:, 4] == 0)[0]\n",
        "        N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]\n",
        "\n",
        "        # Extract boxes, class_ids, scores, and class-specific masks\n",
        "        boxes = detections[:N, :4]\n",
        "        class_ids = detections[:N, 4].astype(np.int32)\n",
        "        scores = detections[:N, 5]\n",
        "        masks = mrcnn_mask[np.arange(N), :, :, class_ids]\n",
        "\n",
        "        # Compute scale and shift to translate coordinates to image domain.\n",
        "        h_scale = image_shape[0] / (window[2] - window[0])\n",
        "        w_scale = image_shape[1] / (window[3] - window[1])\n",
        "        scale = min(h_scale, w_scale)\n",
        "        shift = window[:2]  # y, x\n",
        "        scales = np.array([scale, scale, scale, scale])\n",
        "        shifts = np.array([shift[0], shift[1], shift[0], shift[1]])\n",
        "\n",
        "        # Translate bounding boxes to image domain\n",
        "        boxes = np.multiply(boxes - shifts, scales).astype(np.int32)\n",
        "\n",
        "        # Filter out detections with zero area. Often only happens in early\n",
        "        # stages of training when the network weights are still a bit random.\n",
        "        exclude_ix = np.where(\n",
        "            (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) <= 0)[0]\n",
        "        if exclude_ix.shape[0] > 0:\n",
        "            boxes = np.delete(boxes, exclude_ix, axis=0)\n",
        "            class_ids = np.delete(class_ids, exclude_ix, axis=0)\n",
        "            scores = np.delete(scores, exclude_ix, axis=0)\n",
        "            masks = np.delete(masks, exclude_ix, axis=0)\n",
        "            N = class_ids.shape[0]\n",
        "\n",
        "        # Resize masks to original image size and set boundary threshold.\n",
        "        full_masks = []\n",
        "        for i in range(N):\n",
        "            # Convert neural network mask to full size mask\n",
        "            full_mask = maskutils.unmold_mask(masks[i], boxes[i], image_shape)\n",
        "            full_masks.append(full_mask)\n",
        "        full_masks = np.stack(full_masks, axis=-1)\\\n",
        "            if full_masks else np.empty((0,) + masks.shape[1:3])\n",
        "\n",
        "        return boxes, class_ids, scores, full_masks\n",
        "\n",
        "\n",
        "############################################################\n",
        "#  Data Formatting\n",
        "############################################################\n",
        "\n",
        "def compose_image_meta(image_id, image_shape, window, active_class_ids):\n",
        "    \"\"\"Takes attributes of an image and puts them in one 1D array. Use\n",
        "    parse_image_meta() to parse the values back.\n",
        "    image_id: An int ID of the image. Useful for debugging.\n",
        "    image_shape: [height, width, channels]\n",
        "    window: (y1, x1, y2, x2) in pixels. The area of the image where the real\n",
        "            image is (excluding the padding)\n",
        "    active_class_ids: List of class_ids available in the dataset from which\n",
        "        the image came. Useful if training on images from multiple datasets\n",
        "        where not all classes are present in all datasets.\n",
        "    \"\"\"\n",
        "    meta = np.array(\n",
        "        [image_id] +            # size=1\n",
        "        list(image_shape) +     # size=3\n",
        "        list(window) +          # size=4 (y1, x1, y2, x2) in image cooredinates\n",
        "        list(active_class_ids)  # size=num_classes\n",
        "    )\n",
        "    return meta\n",
        "\n",
        "\n",
        "# Two functions (for Numpy and TF) to parse image_meta tensors.\n",
        "def parse_image_meta(meta):\n",
        "    \"\"\"Parses an image info Numpy array to its components.\n",
        "    See compose_image_meta() for more details.\n",
        "    \"\"\"\n",
        "    image_id = meta[:, 0]\n",
        "    image_shape = meta[:, 1:4]\n",
        "    window = meta[:, 4:8]   # (y1, x1, y2, x2) window of image in in pixels\n",
        "    active_class_ids = meta[:, 8:]\n",
        "    return image_id, image_shape, window, active_class_ids\n",
        "\n",
        "\n",
        "def parse_image_meta_graph(meta):\n",
        "    \"\"\"Parses a tensor that contains image attributes to its components.\n",
        "    See compose_image_meta() for more details.\n",
        "    meta: [batch, meta length] where meta length depends on NUM_CLASSES\n",
        "    \"\"\"\n",
        "    image_id = meta[:, 0]\n",
        "    image_shape = meta[:, 1:4]\n",
        "    window = meta[:, 4:8]\n",
        "    active_class_ids = meta[:, 8:]\n",
        "    return [image_id, image_shape, window, active_class_ids]\n",
        "\n",
        "\n",
        "def mold_image(images, config):\n",
        "    \"\"\"Takes RGB images with 0-255 values and subtraces\n",
        "    the mean pixel and converts it to float. Expects image\n",
        "    colors in RGB order.\n",
        "    \"\"\"\n",
        "    return images.astype(np.float32) - config.MEAN_PIXEL\n",
        "\n",
        "\n",
        "def unmold_image(normalized_images, config):\n",
        "    \"\"\"Takes a image normalized with mold() and returns the original.\"\"\"\n",
        "    return (normalized_images + config.MEAN_PIXEL).astype(np.uint8)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBjcICD6eXlb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a367cc43-22b6-40db-f6b9-5130f91e9873"
      },
      "source": [
        "config = Config()\n",
        "config.NAME = \"coco\"\n",
        "maskrcnn = MaskRCNN(config, '.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ -22.6274,  -11.3137,   22.6274,   11.3137],\n",
            "        [ -16.0000,  -16.0000,   16.0000,   16.0000],\n",
            "        [ -11.3137,  -22.6274,   11.3137,   22.6274],\n",
            "        ...,\n",
            "        [ 597.9613,  778.9807, 1322.0387, 1141.0193],\n",
            "        [ 704.0000,  704.0000, 1216.0000, 1216.0000],\n",
            "        [ 778.9807,  597.9613, 1141.0193, 1322.0387]], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG5LTKDXfknW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "anchor = maskrcnn.get_anchors()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoECAIxzglGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "402763c7-0984-45a0-da93-ae1a563eb2fb"
      },
      "source": [
        "anchor.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([261888, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sa9m_VloKO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = np.zeros((1, 1024, 2048,3))\n",
        "x = maskrcnn.mold_inputs(img)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J733YIJ7vMS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cc72fba-c4e5-4567-dcf8-913901fd61be"
      },
      "source": [
        "x[1]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0, 1024, 2048,    3,  256,    0,  768, 1024,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyW1xVKb4NW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC0u7-H7xErW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "fd5b7910-75d1-46e6-d471-7c028b474743"
      },
      "source": [
        "maskrcnn.to('cuda')\n",
        "w = maskrcnn.detect(img)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:202: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6000, 4])\n",
            "torch.Size([6000])\n",
            "torch.Size([1000, 4])\n",
            "14\n",
            "torch.Size([1, 256, 128, 128])\n",
            "torch.Size([1000, 4])\n",
            "torch.Size([1000])\n",
            "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9f231162e428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmaskrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaskrcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-bfc915fbb910>\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# Run object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrcnn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmolded_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inference'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;31m# Convert to numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bfc915fbb910>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;31m# Detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;31m# output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_rois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrcnn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrcnn_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;31m# Convert boxes to normalized coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-270ed4880bf2>\u001b[0m in \u001b[0;36mdetection_layer\u001b[0;34m(config, rois, mrcnn_class, mrcnn_bbox, image_meta)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_image_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrefine_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrcnn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrcnn_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-270ed4880bf2>\u001b[0m in \u001b[0;36mrefine_detections\u001b[0;34m(rois, probs, deltas, window, config)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mnms_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnms_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m     \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersect1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Keep top detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'nms_keep' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk0wHH_pkDFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a1fde23d-434d-40f4-8c94-ebae52af97eb"
      },
      "source": [
        "print(w[1].shape)\n",
        "len(w[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1000, 4])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9B3nXGOx_sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_pyramid_roi_align(inputs, pool_size, image_shape):\n",
        "    \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid.\n",
        "    Params:\n",
        "    - pool_size: [height, width] of the output pooled regions. Usually [7, 7]\n",
        "    - image_shape: [height, width, channels]. Shape of input image in pixels\n",
        "    Inputs:\n",
        "    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized\n",
        "             coordinates.\n",
        "    - Feature maps: List of feature maps from different levels of the pyramid.\n",
        "                    Each is [batch, channels, height, width]\n",
        "    Output:\n",
        "    Pooled regions in the shape: [num_boxes, height, width, channels].\n",
        "    The width and height are those specific in the pool_shape in the layer\n",
        "    constructor.\n",
        "    \"\"\"\n",
        "\n",
        "    # Currently only supports batchsize 1\n",
        "    for i in range(len(inputs)):\n",
        "        inputs[i] = inputs[i].squeeze(0)\n",
        "\n",
        "    # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n",
        "    boxes = inputs[0]\n",
        "\n",
        "    # Feature Maps. List of feature maps from different level of the\n",
        "    # feature pyramid. Each is [batch, height, width, channels]\n",
        "    feature_maps = inputs[1:]\n",
        "\n",
        "    # Assign each ROI to a level in the pyramid based on the ROI area.\n",
        "    y1, x1, y2, x2 = boxes.chunk(4, dim=1)\n",
        "    h = y2 - y1\n",
        "    w = x2 - x1\n",
        "\n",
        "    # Equation 1 in the Feature Pyramid Networks paper. Account for\n",
        "    # the fact that our coordinates are normalized here.\n",
        "    # e.g. a 224x224 ROI (in pixels) maps to P4\n",
        "    image_area = Variable(torch.FloatTensor([float(image_shape[0]*image_shape[1])]), requires_grad=False)\n",
        "    if boxes.is_cuda:\n",
        "        image_area = image_area.cuda()\n",
        "    roi_level = 4 + log2(torch.sqrt(h*w)/(224.0/torch.sqrt(image_area)))\n",
        "    roi_level = roi_level.round().int()\n",
        "    roi_level = roi_level.clamp(2,5)\n",
        "\n",
        "\n",
        "    # Loop through levels and apply ROI pooling to each. P2 to P5.\n",
        "    pooled = []\n",
        "    box_to_level = []\n",
        "    for i, level in enumerate(range(2, 7)):\n",
        "        ix  = roi_level==level\n",
        "        if not ix.any():\n",
        "            continue\n",
        "        ix = torch.nonzero(ix)[:,0]\n",
        "        level_boxes = boxes[ix.data, :]\n",
        "\n",
        "        # Keep track of which box is mapped to which level\n",
        "        box_to_level.append(ix.data)\n",
        "\n",
        "        # Stop gradient propogation to ROI proposals\n",
        "        level_boxes = level_boxes.detach()\n",
        "\n",
        "        # Crop and Resize\n",
        "        # From Mask R-CNN paper: \"We sample four regular locations, so\n",
        "        # that we can evaluate either max or average pooling. In fact,\n",
        "        # interpolating only a single value at each bin center (without\n",
        "        # pooling) is nearly as effective.\"\n",
        "        #\n",
        "        # Here we use the simplified approach of a single value per bin,\n",
        "        # which is how it's done in tf.crop_and_resize()\n",
        "        # Result: [batch * num_boxes, pool_height, pool_width, channels]\n",
        "        ind = Variable(torch.zeros(level_boxes.size()[0]),requires_grad=False).int()\n",
        "        if level_boxes.is_cuda:\n",
        "            ind = ind.cuda()\n",
        "        feature_maps[i] = feature_maps[i].unsqueeze(0)  #CropAndResizeFunction needs batch dimension\n",
        "        print(pool_size)\n",
        "        print(feature_maps[i].shape)\n",
        "        print(level_boxes.shape)\n",
        "        print(ind.shape)\n",
        "        print(boxes.size(1))\n",
        "        print(ix)\n",
        "        pooled_features = roi_align(feature_maps[i], [level_boxes], (pool_size, pool_size))\n",
        "        pooled.append(pooled_features)\n",
        "\n",
        "    # Pack pooled features into one tensor\n",
        "    pooled = torch.cat(pooled, dim=0)\n",
        "\n",
        "    # Pack box_to_level mapping into one array and add another\n",
        "    # column representing the order of pooled boxes\n",
        "    box_to_level = torch.cat(box_to_level, dim=0)\n",
        "\n",
        "    # Rearrange pooled features to match the order of the original boxes\n",
        "    _, box_to_level = torch.sort(box_to_level)\n",
        "    pooled = pooled[box_to_level, :, :]\n",
        "    return pooled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZIWyqyikgaB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14f91408-92c3-4976-9ddc-c54786bed58c"
      },
      "source": [
        "s = new_pyramid_roi_align([w[1]] + w[0], 14, (1024, 1024, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n",
            "torch.Size([1, 256, 256, 256])\n",
            "torch.Size([378, 4])\n",
            "torch.Size([378])\n",
            "4\n",
            "tensor([434, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516,\n",
            "        517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530,\n",
            "        531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544,\n",
            "        545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
            "        559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572,\n",
            "        573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586,\n",
            "        587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600,\n",
            "        601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614,\n",
            "        615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628,\n",
            "        629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
            "        643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656,\n",
            "        657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670,\n",
            "        671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684,\n",
            "        685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698,\n",
            "        699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712,\n",
            "        713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726,\n",
            "        727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n",
            "        741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754,\n",
            "        755, 756, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826,\n",
            "        827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840,\n",
            "        841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854,\n",
            "        855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868,\n",
            "        869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882,\n",
            "        883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896,\n",
            "        897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910,\n",
            "        911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924,\n",
            "        925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938],\n",
            "       device='cuda:0')\n",
            "14\n",
            "torch.Size([1, 256, 128, 128])\n",
            "torch.Size([235, 4])\n",
            "torch.Size([235])\n",
            "4\n",
            "tensor([ 76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n",
            "         90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
            "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
            "        118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
            "        132, 133, 433, 435, 436, 448, 449, 450, 451, 452, 453, 454, 455, 456,\n",
            "        457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470,\n",
            "        471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484,\n",
            "        485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498,\n",
            "        499, 500, 501, 502, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n",
            "        767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780,\n",
            "        781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794,\n",
            "        795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808,\n",
            "        809, 810, 811, 812, 813, 814, 939, 940, 941, 942, 943, 944, 945, 946,\n",
            "        947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960,\n",
            "        961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974,\n",
            "        975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988,\n",
            "        989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n",
            "       device='cuda:0')\n",
            "14\n",
            "torch.Size([1, 256, 64, 64])\n",
            "torch.Size([387, 4])\n",
            "torch.Size([387])\n",
            "4\n",
            "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75, 134, 135, 136, 137, 138, 139, 140, 141,\n",
            "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
            "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
            "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
            "        184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
            "        198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
            "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
            "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
            "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
            "        254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267,\n",
            "        268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281,\n",
            "        282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
            "        296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
            "        310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
            "        324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
            "        338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
            "        352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365,\n",
            "        366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379,\n",
            "        380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393,\n",
            "        394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407,\n",
            "        408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421,\n",
            "        422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 437, 438, 439,\n",
            "        440, 441, 442, 443, 444, 445, 446, 447, 503], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrIKdDh3ku7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f1c48b4-5403-451e-a65e-5e7dcedf68ac"
      },
      "source": [
        "s.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 256, 14, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "933-imMUmFDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}